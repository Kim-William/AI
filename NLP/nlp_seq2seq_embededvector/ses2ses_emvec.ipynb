{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow[and-cuda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://deeplearningcourses.com/c/deep-learning-advanced-nlp\n",
    "# get the data at: http://www.manythings.org/anki/\n",
    "from __future__ import print_function, division\n",
    "from builtins import range, input\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "import os, sys\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "  import tensorflow.keras.backend as K\n",
    "  if len(K.tensorflow_backend._get_available_gpus()) > 0:\n",
    "    from keras.layers import CuDNNLSTM as LSTM\n",
    "    from keras.layers import CuDNNGRU as GRU\n",
    "except:\n",
    "  pass\n",
    "\n",
    "import tensorflow as tf\n",
    "# import torch\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if not gpus:\n",
    "    print(\"No GPUs found\")\n",
    "else:\n",
    "    print(f\"GPUs available: {gpus}\")\n",
    "    \n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "\n",
    "# some config\n",
    "BATCH_SIZE = 128  # Batch size for training.\n",
    "EPOCHS = 400  # Number of epochs to train for.\n",
    "LATENT_DIM = 256  # Latent dimensionality of the encoding space.\n",
    "NUM_SAMPLES = 15000  # Number of samples to train on.\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# Where we will store the data\n",
    "input_texts = [] # sentence in original language\n",
    "target_texts = [] # sentence in target language\n",
    "target_texts_inputs = [] # sentence in target language offset by 1\n",
    "\n",
    "\n",
    "\n",
    "PATH_EMBEDDING = os.path.join('Data/glove.6B.%sd.txt' % EMBEDDING_DIM)\n",
    "PATH_TOKENIZER_OUTPUT = 'SavedModel_1/tokenizer_output.pickle'\n",
    "PATH_TOKENIZER_INPUT = 'SavedModel_1/tokenizer_input.pickle'\n",
    "\n",
    "PATH_MODEL= 'SavedModel_1/s2s.h5'\n",
    "PATH_TRAIN = 'Data/train.txt'\n",
    "PATH_ENCODER_MODEL = 'SavedModel_1/encoder_model.h5'\n",
    "PATH_DECODER_MODEL = 'SavedModel_1/decoder_model.h5'\n",
    "\n",
    "PATH_SEQUENCE_FILE = 'SavedModel_1/input_sequences.pkl'\n",
    "PATH_TARGET_SEQUENCE_FILE = 'SavedModel_1/target_sequences.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복된 input_text를 추적할 집합을 생성\n",
    "seen_input_texts = set()\n",
    "t=0\n",
    "for line in open(PATH_TRAIN, encoding='utf-8'):\n",
    "    # only keep a limited number of samples\n",
    "    t += 1\n",
    "    if t > NUM_SAMPLES:\n",
    "        break\n",
    "\n",
    "    # input and target are separated by tab\n",
    "    if '\\t' not in line:\n",
    "        continue\n",
    "\n",
    "    line_split = line.rstrip().split('\\t')\n",
    "\n",
    "    # 두 번째 열 (인덱스 1)에 해당하는 input_text\n",
    "    input_text = line_split[0].strip()  # 공백 제거\n",
    "\n",
    "    # 중복된 input_text가 이미 처리되었는지 확인\n",
    "    if input_text in seen_input_texts:\n",
    "        continue  # 중복된 경우 스킵\n",
    "\n",
    "    # 중복되지 않은 input_text는 집합에 추가\n",
    "    seen_input_texts.add(input_text)\n",
    "\n",
    "    # 다섯 번째 열 (인덱스 4)에 해당하는 translation\n",
    "    translation = line_split[1].strip()  # 공백 제거\n",
    "\n",
    "    # split up the input and translation\n",
    "    target_text = translation + ' <eos>'\n",
    "    target_text_input = '<sos> ' + translation\n",
    "\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    target_texts_inputs.append(target_text_input)\n",
    "\n",
    "print(\"num samples:\", len(input_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# tokenize the inputs\n",
    "tokenizer_inputs = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer_inputs.fit_on_texts(input_texts)\n",
    "input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)\n",
    "\n",
    "# Tokenizer 저장\n",
    "import pickle\n",
    "with open(PATH_TOKENIZER_INPUT, 'wb') as handle:\n",
    "    pickle.dump(tokenizer_inputs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # Tokenizer 불러오기\n",
    "# with open('tokenizer.pickle', 'rb') as handle:\n",
    "#     tokenizer_inputs = pickle.load(handle)\n",
    "\n",
    "# get the word to index mapping for input language\n",
    "word2idx_inputs = tokenizer_inputs.word_index\n",
    "print('Found %s unique input tokens.' % len(word2idx_inputs))\n",
    "\n",
    "# determine maximum length input sequence\n",
    "max_len_input = max(len(s) for s in input_sequences)\n",
    "\n",
    "# tokenize the outputs\n",
    "# don't filter out special characters\n",
    "# otherwise <sos> and <eos> won't appear\n",
    "tokenizer_outputs = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
    "tokenizer_outputs.fit_on_texts(target_texts + target_texts_inputs) # inefficient, oh well\n",
    "target_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\n",
    "target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_texts_inputs)\n",
    "\n",
    "# Tokenizer 저장\n",
    "import pickle\n",
    "with open(PATH_TOKENIZER_OUTPUT, 'wb') as handle:\n",
    "    pickle.dump(tokenizer_outputs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# get the word to index mapping for output language\n",
    "word2idx_outputs = tokenizer_outputs.word_index\n",
    "print('Found %s unique output tokens.' % len(word2idx_outputs))\n",
    "\n",
    "# store number of output words for later\n",
    "# remember to add 1 since indexing starts at 1\n",
    "num_words_output = len(word2idx_outputs) + 1\n",
    "\n",
    "# determine maximum length output sequence\n",
    "max_len_target = max(len(s) for s in target_sequences)\n",
    "\n",
    "\n",
    "# pad the sequences\n",
    "encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)\n",
    "print(\"encoder_inputs.shape:\", encoder_inputs.shape)\n",
    "print(\"encoder_inputs[0]:\", encoder_inputs[0])\n",
    "\n",
    "decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=max_len_target, padding='post')\n",
    "print(\"decoder_inputs[0]:\", decoder_inputs[0])\n",
    "print(\"decoder_inputs.shape:\", decoder_inputs.shape)\n",
    "\n",
    "decoder_targets = pad_sequences(target_sequences, maxlen=max_len_target, padding='post')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# store all the pre-trained word vectors\n",
    "print('Loading word vectors...')\n",
    "word2vec = {}\n",
    "with open(PATH_EMBEDDING, encoding='utf-8') as f:\n",
    "  # is just a space-separated text file in the format:\n",
    "  # word vec[0] vec[1] vec[2] ...\n",
    "  for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vec = np.asarray(values[1:], dtype='float32')\n",
    "    word2vec[word] = vec\n",
    "print('Found %s word vectors.' % len(word2vec))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# prepare embedding matrix\n",
    "print('Filling pre-trained embeddings...')\n",
    "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word2idx_inputs.items():\n",
    "  if i < MAX_NUM_WORDS:\n",
    "    embedding_vector = word2vec.get(word)\n",
    "    if embedding_vector is not None:\n",
    "      # words not found in embedding index will be all zeros.\n",
    "      embedding_matrix[i] = embedding_vector\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create embedding layer\n",
    "embedding_layer = Embedding(\n",
    "  num_words,\n",
    "  EMBEDDING_DIM,\n",
    "  weights=[embedding_matrix],\n",
    "  input_length=max_len_input,\n",
    "  # trainable=True\n",
    ")\n",
    "\n",
    "\n",
    "# create targets, since we cannot use sparse\n",
    "# categorical cross entropy when we have sequences\n",
    "decoder_targets_one_hot = np.zeros(\n",
    "  (\n",
    "    len(input_texts),\n",
    "    max_len_target,\n",
    "    num_words_output\n",
    "  ),\n",
    "  dtype='float32'\n",
    ")\n",
    "\n",
    "# assign the values\n",
    "for i, d in enumerate(decoder_targets):\n",
    "  for t, word in enumerate(d):\n",
    "    if word != 0:\n",
    "      decoder_targets_one_hot[i, t, word] = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### build the model #####\n",
    "encoder_inputs_placeholder = Input(shape=(max_len_input,))\n",
    "x = embedding_layer(encoder_inputs_placeholder)\n",
    "encoder = LSTM(\n",
    "  LATENT_DIM,\n",
    "  return_state=True,\n",
    "  # dropout=0.5 # dropout not available on gpu\n",
    ")\n",
    "encoder_outputs, h, c = encoder(x)\n",
    "# encoder_outputs, h = encoder(x) #gru\n",
    "\n",
    "# keep only the states to pass into decoder\n",
    "encoder_states = [h, c]\n",
    "# encoder_states = [state_h] # gru\n",
    "\n",
    "# Set up the decoder, using [h, c] as initial state.\n",
    "decoder_inputs_placeholder = Input(shape=(max_len_target,))\n",
    "\n",
    "# this word embedding will not use pre-trained vectors\n",
    "# although you could\n",
    "decoder_embedding = Embedding(num_words_output, EMBEDDING_DIM)\n",
    "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
    "\n",
    "# since the decoder is a \"to-many\" model we want to have\n",
    "# return_sequences=True\n",
    "decoder_lstm = LSTM(\n",
    "  LATENT_DIM,\n",
    "  return_sequences=True,\n",
    "  return_state=True,\n",
    "  # dropout=0.5 # dropout not available on gpu\n",
    ")\n",
    "decoder_outputs, _, _ = decoder_lstm(\n",
    "  decoder_inputs_x,\n",
    "  initial_state=encoder_states\n",
    ")\n",
    "\n",
    "# decoder_outputs, _ = decoder_gru(\n",
    "#   decoder_inputs_x,\n",
    "#   initial_state=encoder_states\n",
    "# )\n",
    "\n",
    "# final dense layer for predictions\n",
    "decoder_dense = Dense(num_words_output, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Create the model object\n",
    "model = Model([encoder_inputs_placeholder, decoder_inputs_placeholder], decoder_outputs)\n",
    "\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "  # both are of shape N x T x K\n",
    "  mask = K.cast(y_true > 0, dtype='float32')\n",
    "  out = mask * y_true * K.log(y_pred)\n",
    "  return -K.sum(out) / K.sum(mask)\n",
    "\n",
    "\n",
    "def acc(y_true, y_pred):\n",
    "  # both are of shape N x T x K\n",
    "  targ = K.argmax(y_true, axis=-1)\n",
    "  pred = K.argmax(y_pred, axis=-1)\n",
    "  correct = K.cast(K.equal(targ, pred), dtype='float32')\n",
    "\n",
    "  # 0 is padding, don't include those\n",
    "  mask = K.cast(K.greater(targ, 0), dtype='float32')\n",
    "  n_correct = K.sum(mask * correct)\n",
    "  n_total = K.sum(mask)\n",
    "  return n_correct / n_total\n",
    "\n",
    "model.compile(optimizer='adam', loss=custom_loss, metrics=[acc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "r = model.fit(\n",
    "  [encoder_inputs, decoder_inputs], decoder_targets_one_hot,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  epochs=EPOCHS,\n",
    "  validation_split=0.2,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 가중치 비교\n",
    "original_weights = model.get_weights()\n",
    "\n",
    "# 모델을 저장\n",
    "model.save(PATH_MODEL)\n",
    "\n",
    "# 모델을 불러옴\n",
    "model_loaded = load_model(PATH_MODEL, custom_objects={'custom_loss': custom_loss, 'acc': acc})\n",
    "\n",
    "# 불러온 모델의 가중치\n",
    "loaded_weights = model_loaded.get_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 비교\n",
    "for original, loaded in zip(original_weights, loaded_weights):\n",
    "    if np.array_equal(original, loaded):\n",
    "        print(\"가중치 일치\")\n",
    "    else:\n",
    "        print(\"가중치 불일치\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot some data\n",
    "plt.plot(r.history['loss'], label='loss')\n",
    "plt.plot(r.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# accuracies\n",
    "plt.plot(r.history['acc'], label='acc')\n",
    "plt.plot(r.history['val_acc'], label='val_acc')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##### Make predictions #####\n",
    "# As with the poetry example, we need to create another model\n",
    "# that can take in the RNN state and previous word as input\n",
    "# and accept a T=1 sequence.\n",
    "\n",
    "# The encoder will be stand-alone\n",
    "# From this we will get our initial decoder hidden state\n",
    "encoder_model = Model(encoder_inputs_placeholder, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(LATENT_DIM,))\n",
    "decoder_state_input_c = Input(shape=(LATENT_DIM,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "# decoder_states_inputs = [decoder_state_input_h] # gru\n",
    "\n",
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n",
    "\n",
    "# this time, we want to keep the states too, to be output\n",
    "# by our sampling model\n",
    "decoder_outputs, h, c = decoder_lstm(\n",
    "  decoder_inputs_single_x,\n",
    "  initial_state=decoder_states_inputs\n",
    ")\n",
    "# decoder_outputs, state_h = decoder_lstm(\n",
    "#   decoder_inputs_single_x,\n",
    "#   initial_state=decoder_states_inputs\n",
    "# ) #gru\n",
    "decoder_states = [h, c]\n",
    "# decoder_states = [h] # gru\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# The sampling model\n",
    "# inputs: y(t-1), h(t-1), c(t-1)\n",
    "# outputs: y(t), h(t), c(t)\n",
    "decoder_model = Model(\n",
    "  [decoder_inputs_single] + decoder_states_inputs, \n",
    "  [decoder_outputs] + decoder_states\n",
    ")\n",
    "\n",
    "# map indexes back into real words\n",
    "# so we can view the results\n",
    "idx2word_eng = {v:k for k, v in word2idx_inputs.items()}\n",
    "idx2word_trans = {v:k for k, v in word2idx_outputs.items()}\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "  # Encode the input as state vectors.\n",
    "  states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "  # Generate empty target sequence of length 1.\n",
    "  target_seq = np.zeros((1, 1))\n",
    "\n",
    "  # Populate the first character of target sequence with the start character.\n",
    "  # NOTE: tokenizer lower-cases all words\n",
    "  target_seq[0, 0] = word2idx_outputs['<sos>']\n",
    "\n",
    "  # if we get this we break\n",
    "  eos = word2idx_outputs['<eos>']\n",
    "\n",
    "  # Create the translation\n",
    "  output_sentence = []\n",
    "  for _ in range(max_len_target):\n",
    "    output_tokens, h, c = decoder_model.predict(\n",
    "      [target_seq] + states_value\n",
    "    )\n",
    "    # output_tokens, h = decoder_model.predict(\n",
    "    #     [target_seq] + states_value\n",
    "    # ) # gru\n",
    "\n",
    "    # Get next word\n",
    "    idx = np.argmax(output_tokens[0, 0, :])\n",
    "\n",
    "    # End sentence of EOS\n",
    "    if eos == idx:\n",
    "      break\n",
    "\n",
    "    word = ''\n",
    "    if idx > 0:\n",
    "      word = idx2word_trans[idx]\n",
    "      output_sentence.append(word)\n",
    "\n",
    "    # Update the decoder input\n",
    "    # which is just the word just generated\n",
    "    target_seq[0, 0] = idx\n",
    "\n",
    "    # Update states\n",
    "    states_value = [h, c]\n",
    "    # states_value = [h] # gru\n",
    "\n",
    "  return ' '.join(output_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder_model.save(PATH_ENCODER_MODEL)\n",
    "decoder_model.save(PATH_DECODER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  # Do some test translations\n",
    "i = np.random.choice(len(input_texts))\n",
    "input_seq = encoder_inputs[i:i+1]\n",
    "translation = decode_sequence(input_seq)\n",
    "print('-')\n",
    "print('Input:', input_texts[i])\n",
    "print('Translation:', translation)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
