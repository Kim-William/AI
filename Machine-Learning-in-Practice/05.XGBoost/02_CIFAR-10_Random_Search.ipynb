{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-14 20:59:47.569954: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-14 20:59:47.578628: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-14 20:59:47.581193: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-14 20:59:47.587963: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-14 20:59:47.973541: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woong/myenv/lib/python3.9/site-packages/xgboost/core.py:158: UserWarning: [20:59:55] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'tree_method': 'hist', 'subsample': 0.8, 'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.1, 'gamma': 0.1, 'device': 'cuda', 'colsample_bytree': 0.7}\n",
      "Test accuracy: 0.38\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Flatten the images (convert 32x32x3 images into 1D vectors)\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_flat = scaler.fit_transform(X_train_flat)\n",
    "X_test_flat = scaler.transform(X_test_flat)\n",
    "\n",
    "# Optional: Use a smaller subset of the data to speed up the tuning process\n",
    "X_train_subset, _, y_train_subset, _ = train_test_split(X_train_flat, y_train, test_size=0.95, random_state=42)\n",
    "X_test_subset, _, y_test_subset, _ = train_test_split(X_test_flat, y_test, test_size=0.95, random_state=42)\n",
    "\n",
    "# Convert the labels into a 1D array\n",
    "y_train_subset = y_train_subset.ravel()\n",
    "y_test_subset = y_test_subset.ravel()\n",
    "\n",
    "# Create an XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(eval_metric='mlogloss')\n",
    "\n",
    "# Set the hyperparameter grid\n",
    "param_dist = {\n",
    "    'max_depth': [3, 5, 7],           # Maximum depth of the trees\n",
    "    'learning_rate': [0.01, 0.1, 0.3],# Learning rate\n",
    "    'n_estimators': [50, 100, 200],   # Number of boosting rounds\n",
    "    'gamma': [0, 0.1, 0.3],           # Minimum loss reduction required for further partitioning\n",
    "    'subsample': [0.7, 0.8, 1.0],     # Fraction of samples used per tree\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0], # Fraction of features used per tree,\n",
    "    'tree_method': ['hist'],          # Use 'hist' method for tree building\n",
    "    'device': ['cuda']                # Use GPU (CUDA)\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    xgb_model, param_distributions=param_dist, n_iter=10, scoring='accuracy', cv=3, verbose=1, random_state=42\n",
    ")\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "random_search.fit(X_train_subset, y_train_subset)\n",
    "\n",
    "# Output the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "\n",
    "# Evaluate performance on the test set\n",
    "y_pred = random_search.best_estimator_.predict(X_test_subset)\n",
    "accuracy = accuracy_score(y_test_subset, y_pred)\n",
    "print(f\"Test accuracy: {accuracy:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
