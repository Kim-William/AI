{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4abd78e1",
   "metadata": {},
   "source": [
    "I have successfully performed cross-validation and grid search for various regression models and obtained RMSE and MAE scores. Now, we can run some key evaluations to further analyse the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b334d7db",
   "metadata": {},
   "source": [
    "Visualising Predictions vs. Actual Values: You can create scatter plots or line plots to visualize how well each model's predictions align with the actual target values. This can give you a qualitative sense of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c09526",
   "metadata": {},
   "source": [
    "Stat tests to compare all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97026c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/4036861040.py:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({'Model': model_name, 'RMSE': avg_rmse, 'MAE': avg_mae}, ignore_index=True)\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/4036861040.py:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({'Model': model_name, 'RMSE': avg_rmse, 'MAE': avg_mae}, ignore_index=True)\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/4036861040.py:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({'Model': model_name, 'RMSE': avg_rmse, 'MAE': avg_mae}, ignore_index=True)\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/4036861040.py:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({'Model': model_name, 'RMSE': avg_rmse, 'MAE': avg_mae}, ignore_index=True)\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/4036861040.py:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({'Model': model_name, 'RMSE': avg_rmse, 'MAE': avg_mae}, ignore_index=True)\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/4036861040.py:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({'Model': model_name, 'RMSE': avg_rmse, 'MAE': avg_mae}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Model      RMSE       MAE\n",
      "0         Linear Regression  1.067225  0.305496\n",
      "1             Random Forest  0.255806  0.193998\n",
      "2         Gradient Boosting  0.263997  0.198448\n",
      "3                     Ridge  0.394622  0.326558\n",
      "4                     Lasso  0.392976  0.333159\n",
      "5                   XGBoost  0.264377  0.198081\n",
      "6  Neural Network Regressor  0.330178  0.241556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/4036861040.py:65: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "\n",
    "# Initialise the models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n",
    "    \"Ridge\": Ridge(alpha=1.0, random_state=42),\n",
    "    \"Lasso\": Lasso(alpha=0.1, random_state=42),\n",
    "    \"XGBoost\": xgb.XGBRegressor(objective ='reg:squarederror', random_state=42)\n",
    "}\n",
    "\n",
    "# Initialise results DataFrame\n",
    "results_df = pd.DataFrame(columns=['Model', 'RMSE', 'MAE'])\n",
    "\n",
    "# Perform cross-validation and store results\n",
    "for model_name, model in models.items():\n",
    "    # Calculate cross-validated RMSE\n",
    "    neg_mse_scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    rmse_scores = np.sqrt(-neg_mse_scores)\n",
    "    avg_rmse = np.mean(rmse_scores)\n",
    "\n",
    "    # Calculate cross-validated MAE\n",
    "    mae_scores = -cross_val_score(model, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "    avg_mae = np.mean(mae_scores)\n",
    "\n",
    "    # Append results to the DataFrame\n",
    "    results_df = results_df.append({'Model': model_name, 'RMSE': avg_rmse, 'MAE': avg_mae}, ignore_index=True)\n",
    "\n",
    "# Hyperparameter grid for NN\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(100,), (100, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'learning_rate_init': [0.001, 0.01],\n",
    "}\n",
    "\n",
    "# Initialise the NN Regressor model\n",
    "nn_model = MLPRegressor(max_iter=2000, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Perform Grid Search for NN\n",
    "grid_search = GridSearchCV(nn_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_scaled, y)\n",
    "best_nn_model = grid_search.best_estimator_\n",
    "\n",
    "# Compute RMSE and MAE for NN model and add it to the results DataFrame\n",
    "nn_rmse = np.sqrt(-grid_search.best_score_)\n",
    "nn_mae_scores = -cross_val_score(best_nn_model, X_scaled, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "nn_avg_mae = np.mean(nn_mae_scores)\n",
    "\n",
    "# Append the NN results to the results DataFrame\n",
    "results_df = results_df.append({\n",
    "    'Model': 'Neural Network Regressor',\n",
    "    'RMSE': nn_rmse,\n",
    "    'MAE': nn_avg_mae\n",
    "}, ignore_index=True)\n",
    "\n",
    "# Print the updated results table\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8a6d78",
   "metadata": {},
   "source": [
    "When evaluating model performance:\n",
    "\n",
    "The model with the lowest RMSE and MAE has the least prediction error on average. However, if a model is too complex, it might perform exceptionally well on the training set but poorly on unseen data (overfitting).\n",
    "\n",
    "Comparative Analysis:\n",
    "\n",
    "Linear Regression: the most basic model and often serves as a benchmark. If complex models don't perform significantly better than linear regression, it could be a sign that the added complexity isn't beneficial.\n",
    "\n",
    "Random Forest & Gradient Boosting: These are ensemble models. In our results, they have much lower RMSE compared to the linear models, suggesting they are capturing nonlinear patterns in the data better.\n",
    "\n",
    "Ridge & Lasso: These are regularised linear regression models. The results show slightly higher error metrics than plain linear regression. This could indicate that the regularised versions aren't as beneficial for our dataset, or the hyperparameters need tuning.\n",
    "\n",
    "XGBoost: An advanced gradient boosting algorithm. It seems to be performing comparably to the Random Forest and Gradient Boosting models in our case.\n",
    "\n",
    "Neural Network Regressor: NNs are highly flexible models. Their performance can vary significantly with architecture and hyperparameters. Our neural network is performing better than the linear models but not as good as the tree-based models. The NN is capturing more complex relationships in the dataset than simpler models like Linear Regression, Ridge, or Lasso. This could be due to non-linear patterns in the data which linear models can't capture as effectively. Even though the NN is flexible, in this specific instance, it hasn't outperformed tree-based models like Random Forest, Gradient Boosting, or XGBoost. Tree-based models are adept at handling non-linearity, interaction effects, and can be more robust to certain types of data structures. It's also possible that the tree-based models are inherently better suited for this dataset, or that the specific neural network architecture and hyperparameters chosen weren't optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fce316d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAEI 1km2 ID: 0.032061205274607665\n",
      "GRID_ExactCut_ID: 0.03633522462479362\n",
      "Easting_y: 0.03162297042149062\n",
      "Northing_y: 0.012987770141891427\n",
      "SO2: 0.062056202208094775\n",
      "NMVOC: 0.42571840196920224\n",
      "NH3: 0.02355037323216946\n",
      "CO: 0.04887203651307892\n",
      "CH4: 0.04969606085089379\n",
      "N2O: 0.02809915907548689\n",
      "Cd: 0.025181446314902604\n",
      "Hg: 0.012835127935211334\n",
      "Pb: 0.058198675432969596\n",
      "BaP: 0.050208850514976235\n",
      "PCB: 0.0317355005801904\n",
      "HCl: 4.34364484089935e-05\n",
      "PM10_cox_lag1: 0.03585803273884311\n",
      "PM10_cox_lag2: 0.03493952572278818\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_train, y_train are your training data\n",
    "models[\"Random Forest\"].fit(X_train, y_train)\n",
    "\n",
    "feature_names = X.columns\n",
    "# Get feature importances\n",
    "importances = models[\"Random Forest\"].feature_importances_\n",
    "\n",
    "# Assuming feature_names is a list of your feature names\n",
    "for feature, importance in zip(feature_names, importances):\n",
    "    print(f\"{feature}: {importance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8a951f",
   "metadata": {},
   "source": [
    "The importance value of each feature tells us how much that specific feature contributes to the model's decisions. The higher the importance, the more influential the feature is in determining the model's predictions.\n",
    "\n",
    "NMVOC has the highest feature importance at approximately 0.4257. This suggests that the Random Forest model considers NMVOC to be the most informative feature when making predictions.\n",
    "Other features with relatively high importances include SO2, CO, CH4, BaP, Pb, and GRID_ExactCut_ID.\n",
    "\n",
    "HCl has the smallest importance value, almost 0. \n",
    "This means it has very minimal influence in the model's decisions.\n",
    "However, feature importances in a Random Forest model don't tell you about the relationship direction (whether the relationship is positive or negative), but only the strength or magnitude of the influence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713a9fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3cbb8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6311366570845751\n"
     ]
    }
   ],
   "source": [
    "# Modify the instantiation\n",
    "models[\"Random Forest\"] = RandomForestRegressor(oob_score=True, random_state=42)\n",
    "\n",
    "# Train the model (assuming X_train and y_train are your training data)\n",
    "models[\"Random Forest\"].fit(X_train, y_train)\n",
    "\n",
    "# Access the OOB score\n",
    "print(models[\"Random Forest\"].oob_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ffcc6d",
   "metadata": {},
   "source": [
    "The oob_score_ is the Out-Of-Bag (OOB) score for a Random Forest model. It's a way to measure the prediction accuracy of a Random Forest using the samples that were not included (left out) during the construction (bootstrap sampling) of individual trees. This means that the model's accuracy on the out-of-bag samples is approximately 63.11%. This is a pretty good score.\n",
    "It provides a way to get an estimate of the model's performance without needing a separate validation set, which can be especially valuable if you have limited data.\n",
    "It's a form of cross-validation that comes with the way Random Forests are constructed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dd2659",
   "metadata": {},
   "source": [
    "Further evals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c354ba03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/2716122905.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_results_df = val_results_df.append({'Model': model_name, 'RMSE': rmse_val, 'MAE': mae_val}, ignore_index=True)\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/2716122905.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_results_df = val_results_df.append({'Model': model_name, 'RMSE': rmse_val, 'MAE': mae_val}, ignore_index=True)\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/2716122905.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_results_df = val_results_df.append({'Model': model_name, 'RMSE': rmse_val, 'MAE': mae_val}, ignore_index=True)\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/2716122905.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_results_df = val_results_df.append({'Model': model_name, 'RMSE': rmse_val, 'MAE': mae_val}, ignore_index=True)\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/2716122905.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_results_df = val_results_df.append({'Model': model_name, 'RMSE': rmse_val, 'MAE': mae_val}, ignore_index=True)\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/2716122905.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_results_df = val_results_df.append({'Model': model_name, 'RMSE': rmse_val, 'MAE': mae_val}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE for best model: 0.24275280057627383\n",
      "Test MAE for best model: 0.17751598684054395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/2716122905.py:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_results_df = val_results_df.append({\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.347094</td>\n",
       "      <td>0.274564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.263417</td>\n",
       "      <td>0.198446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.265198</td>\n",
       "      <td>0.197990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.397263</td>\n",
       "      <td>0.324250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.396276</td>\n",
       "      <td>0.333569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.263830</td>\n",
       "      <td>0.195503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Neural Network Regressor</td>\n",
       "      <td>0.300518</td>\n",
       "      <td>0.228293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model      RMSE       MAE\n",
       "0         Linear Regression  0.347094  0.274564\n",
       "1             Random Forest  0.263417  0.198446\n",
       "2         Gradient Boosting  0.265198  0.197990\n",
       "3                     Ridge  0.397263  0.324250\n",
       "4                     Lasso  0.396276  0.333569\n",
       "5                   XGBoost  0.263830  0.195503\n",
       "6  Neural Network Regressor  0.300518  0.228293"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/2716122905.py:88: FutureWarning: this method is deprecated in favour of `Styler.format(precision=..)`\n",
      "  display(val_results_df.style.set_precision(4).set_caption(\"Validation Results\").hide_index())\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/2716122905.py:88: FutureWarning: this method is deprecated in favour of `Styler.hide(axis='index')`\n",
      "  display(val_results_df.style.set_precision(4).set_caption(\"Validation Results\").hide_index())\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_cf694\">\n",
       "  <caption>Validation Results</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_cf694_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_cf694_level0_col1\" class=\"col_heading level0 col1\" >RMSE</th>\n",
       "      <th id=\"T_cf694_level0_col2\" class=\"col_heading level0 col2\" >MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_cf694_row0_col0\" class=\"data row0 col0\" >Linear Regression</td>\n",
       "      <td id=\"T_cf694_row0_col1\" class=\"data row0 col1\" >0.3471</td>\n",
       "      <td id=\"T_cf694_row0_col2\" class=\"data row0 col2\" >0.2746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf694_row1_col0\" class=\"data row1 col0\" >Random Forest</td>\n",
       "      <td id=\"T_cf694_row1_col1\" class=\"data row1 col1\" >0.2634</td>\n",
       "      <td id=\"T_cf694_row1_col2\" class=\"data row1 col2\" >0.1984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf694_row2_col0\" class=\"data row2 col0\" >Gradient Boosting</td>\n",
       "      <td id=\"T_cf694_row2_col1\" class=\"data row2 col1\" >0.2652</td>\n",
       "      <td id=\"T_cf694_row2_col2\" class=\"data row2 col2\" >0.1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf694_row3_col0\" class=\"data row3 col0\" >Ridge</td>\n",
       "      <td id=\"T_cf694_row3_col1\" class=\"data row3 col1\" >0.3973</td>\n",
       "      <td id=\"T_cf694_row3_col2\" class=\"data row3 col2\" >0.3243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf694_row4_col0\" class=\"data row4 col0\" >Lasso</td>\n",
       "      <td id=\"T_cf694_row4_col1\" class=\"data row4 col1\" >0.3963</td>\n",
       "      <td id=\"T_cf694_row4_col2\" class=\"data row4 col2\" >0.3336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf694_row5_col0\" class=\"data row5 col0\" >XGBoost</td>\n",
       "      <td id=\"T_cf694_row5_col1\" class=\"data row5 col1\" >0.2638</td>\n",
       "      <td id=\"T_cf694_row5_col2\" class=\"data row5 col2\" >0.1955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_cf694_row6_col0\" class=\"data row6 col0\" >Neural Network Regressor</td>\n",
       "      <td id=\"T_cf694_row6_col1\" class=\"data row6 col1\" >0.3005</td>\n",
       "      <td id=\"T_cf694_row6_col2\" class=\"data row6 col2\" >0.2283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f80d20cc250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Splitting the data\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # 20% for testing\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 60% for training, 20% for validation\n",
    "\n",
    "# Initialise the models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n",
    "    \"Ridge\": Ridge(alpha=1.0, random_state=42),\n",
    "    \"Lasso\": Lasso(alpha=0.1, random_state=42),\n",
    "    \"XGBoost\": xgb.XGBRegressor(objective ='reg:squarederror', random_state=42)\n",
    "}\n",
    "\n",
    "# Results dataframe for validation\n",
    "val_results_df = pd.DataFrame(columns=['Model', 'RMSE', 'MAE'])\n",
    "\n",
    "# Train and validate the models\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    val_predictions = model.predict(X_val)\n",
    "    \n",
    "    rmse_val = np.sqrt(mean_squared_error(y_val, val_predictions))\n",
    "    mae_val = mean_absolute_error(y_val, val_predictions)\n",
    "\n",
    "    val_results_df = val_results_df.append({'Model': model_name, 'RMSE': rmse_val, 'MAE': mae_val}, ignore_index=True)\n",
    "\n",
    "# Scaling for NN\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Hyperparameter grid for NN\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(100,), (100, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'learning_rate_init': [0.001, 0.01],\n",
    "}\n",
    "\n",
    "# Initialise the NN Regressor model\n",
    "nn_model = MLPRegressor(max_iter=2000, random_state=42)\n",
    "\n",
    "# Grid Search for NN\n",
    "grid_search = GridSearchCV(nn_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "best_nn_model = grid_search.best_estimator_\n",
    "\n",
    "val_predictions_nn = best_nn_model.predict(X_val_scaled)\n",
    "rmse_val_nn = np.sqrt(mean_squared_error(y_val, val_predictions_nn))\n",
    "mae_val_nn = mean_absolute_error(y_val, val_predictions_nn)\n",
    "\n",
    "# Append the NN validation results\n",
    "val_results_df = val_results_df.append({\n",
    "    'Model': 'Neural Network Regressor',\n",
    "    'RMSE': rmse_val_nn,\n",
    "    'MAE': mae_val_nn\n",
    "}, ignore_index=True)\n",
    "\n",
    "# Final test evaluation\n",
    "# Choose best model based on validation RMSE/MAE \n",
    "\n",
    "best_model = models[\"Random Forest\"]\n",
    "test_predictions = best_model.predict(X_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "mae_test = mean_absolute_error(y_test, test_predictions)\n",
    "\n",
    "print(\"Test RMSE for best model:\", rmse_test)\n",
    "print(\"Test MAE for best model:\", mae_test)\n",
    "\n",
    "# print(val_results_df)\n",
    "import IPython\n",
    "\n",
    "# Display the results\n",
    "display(val_results_df)\n",
    "\n",
    "# To display with more formatting (e.g., rounding the numbers), we can use:\n",
    "display(val_results_df.style.set_precision(4).set_caption(\"Validation Results\").hide_index())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f45ba7",
   "metadata": {},
   "source": [
    "To idetify 'best model' adding logic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57c6f096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/2336692573.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_results_df = val_results_df.append({'Model': model_name, 'RMSE': rmse_val, 'MAE': mae_val}, ignore_index=True)\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/2336692573.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_results_df = val_results_df.append({'Model': model_name, 'RMSE': rmse_val, 'MAE': mae_val}, ignore_index=True)\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/2336692573.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_results_df = val_results_df.append({'Model': model_name, 'RMSE': rmse_val, 'MAE': mae_val}, ignore_index=True)\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/2336692573.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_results_df = val_results_df.append({'Model': model_name, 'RMSE': rmse_val, 'MAE': mae_val}, ignore_index=True)\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/2336692573.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_results_df = val_results_df.append({'Model': model_name, 'RMSE': rmse_val, 'MAE': mae_val}, ignore_index=True)\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/2336692573.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_results_df = val_results_df.append({'Model': model_name, 'RMSE': rmse_val, 'MAE': mae_val}, ignore_index=True)\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/2336692573.py:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_results_df = val_results_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model based on Validation RMSE: Random Forest\n",
      "Test RMSE for best model: 0.24589050844471486\n",
      "Test MAE for best model: 0.18008911169668682\n",
      "                      Model      RMSE       MAE\n",
      "0         Linear Regression  0.347094  0.274564\n",
      "1             Random Forest  0.263417  0.198446\n",
      "2         Gradient Boosting  0.265198  0.197990\n",
      "3                     Ridge  0.397263  0.324250\n",
      "4                     Lasso  0.396276  0.333569\n",
      "5                   XGBoost  0.263830  0.195503\n",
      "6  Neural Network Regressor  0.300518  0.228293\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Splitting the data\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # 20% for testing\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 60% for training, 20% for validation\n",
    "\n",
    "# Initialise the models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n",
    "    \"Ridge\": Ridge(alpha=1.0, random_state=42),\n",
    "    \"Lasso\": Lasso(alpha=0.1, random_state=42),\n",
    "    \"XGBoost\": xgb.XGBRegressor(objective ='reg:squarederror', random_state=42)\n",
    "}\n",
    "\n",
    "# Results dataframe for validation\n",
    "val_results_df = pd.DataFrame(columns=['Model', 'RMSE', 'MAE'])\n",
    "\n",
    "# Train and validate the models\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    val_predictions = model.predict(X_val)\n",
    "    \n",
    "    rmse_val = np.sqrt(mean_squared_error(y_val, val_predictions))\n",
    "    mae_val = mean_absolute_error(y_val, val_predictions)\n",
    "\n",
    "    val_results_df = val_results_df.append({'Model': model_name, 'RMSE': rmse_val, 'MAE': mae_val}, ignore_index=True)\n",
    "\n",
    "# Scaling for NN\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Hyperparameter grid for NN\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(100,), (100, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'learning_rate_init': [0.001, 0.01],\n",
    "}\n",
    "\n",
    "# Initialise the NN Regressor model\n",
    "nn_model = MLPRegressor(max_iter=2000, random_state=42)\n",
    "\n",
    "# Grid Search for NN\n",
    "grid_search = GridSearchCV(nn_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "best_nn_model = grid_search.best_estimator_\n",
    "\n",
    "val_predictions_nn = best_nn_model.predict(X_val_scaled)\n",
    "rmse_val_nn = np.sqrt(mean_squared_error(y_val, val_predictions_nn))\n",
    "mae_val_nn = mean_absolute_error(y_val, val_predictions_nn)\n",
    "\n",
    "# Append the NN validation results\n",
    "val_results_df = val_results_df.append({\n",
    "    'Model': 'Neural Network Regressor',\n",
    "    'RMSE': rmse_val_nn,\n",
    "    'MAE': mae_val_nn\n",
    "}, ignore_index=True)\n",
    "\n",
    "# After getting the validation results\n",
    "best_model_name = val_results_df.loc[val_results_df['RMSE'].idxmin()]['Model']\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "# Now, you can evaluate the best model on the test set:\n",
    "best_model.fit(X_temp, y_temp)  # First, we retrain on both training and validation combined\n",
    "test_predictions = best_model.predict(X_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "mae_test = mean_absolute_error(y_test, test_predictions)\n",
    "\n",
    "print(f\"Best Model based on Validation RMSE: {best_model_name}\")\n",
    "print(\"Test RMSE for best model:\", rmse_test)\n",
    "print(\"Test MAE for best model:\", mae_test)\n",
    "\n",
    "print(val_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc8bc2d",
   "metadata": {},
   "source": [
    "The Random Forest model has been identified as the best model based on the validation RMSE.\n",
    "On the test set, this Random Forest model achieved an RMSE of approximately 0.2459 and an MAE of approximately 0.1801.\n",
    "When comparing this test performance with its validation RMSE of 0.2634, the Random Forest model's performance is consistent and even slightly better on the test set, which is a good sign of generalisation.\n",
    "\n",
    "Comparing across all models:\n",
    "\n",
    "The Random Forest, XGBoost, and Gradient Boosting models are the top three performers in terms of RMSE. Their performance is closely matched, with Random Forest leading by a small margin in the validation results.\n",
    "The Neural Network Regressor, though not matching the tree-based models, still outperforms the linear models (Linear Regression, Ridge, and Lasso) in terms of RMSE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
