{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d30f102",
   "metadata": {},
   "source": [
    "I have successfully performed cross-validation and grid search for various regression models and obtained RMSE and MAE scores. Now, we can run some key evaluations to further analyse the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5803c8c5",
   "metadata": {},
   "source": [
    "Visualising Predictions vs. Actual Values: For example, we can create scatter plots to visualise how well each model's predictions align with the PM10. This can give a qualitative sense of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e223bd2",
   "metadata": {},
   "source": [
    "Stat tests to compare all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cfbfd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/4036861040.py:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({'Model': model_name, 'RMSE': avg_rmse, 'MAE': avg_mae}, ignore_index=True)\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/4036861040.py:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({'Model': model_name, 'RMSE': avg_rmse, 'MAE': avg_mae}, ignore_index=True)\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/4036861040.py:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({'Model': model_name, 'RMSE': avg_rmse, 'MAE': avg_mae}, ignore_index=True)\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/4036861040.py:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({'Model': model_name, 'RMSE': avg_rmse, 'MAE': avg_mae}, ignore_index=True)\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/4036861040.py:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({'Model': model_name, 'RMSE': avg_rmse, 'MAE': avg_mae}, ignore_index=True)\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/4036861040.py:38: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({'Model': model_name, 'RMSE': avg_rmse, 'MAE': avg_mae}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Model      RMSE       MAE\n",
      "0         Linear Regression  1.067225  0.305496\n",
      "1             Random Forest  0.255806  0.193998\n",
      "2         Gradient Boosting  0.263997  0.198448\n",
      "3                     Ridge  0.394622  0.326558\n",
      "4                     Lasso  0.392976  0.333159\n",
      "5                   XGBoost  0.264377  0.198081\n",
      "6  Neural Network Regressor  0.330178  0.241556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/4036861040.py:65: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  results_df = results_df.append({\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "\n",
    "# Initialise the models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n",
    "    \"Ridge\": Ridge(alpha=1.0, random_state=42),\n",
    "    \"Lasso\": Lasso(alpha=0.1, random_state=42),\n",
    "    \"XGBoost\": xgb.XGBRegressor(objective ='reg:squarederror', random_state=42)\n",
    "}\n",
    "\n",
    "# Initialise results DataFrame\n",
    "results_df = pd.DataFrame(columns=['Model', 'RMSE', 'MAE'])\n",
    "\n",
    "# Perform cross-validation and store results\n",
    "for model_name, model in models.items():\n",
    "    # Calculate cross-validated RMSE\n",
    "    neg_mse_scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "    rmse_scores = np.sqrt(-neg_mse_scores)\n",
    "    avg_rmse = np.mean(rmse_scores)\n",
    "\n",
    "    # Calculate cross-validated MAE\n",
    "    mae_scores = -cross_val_score(model, X, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "    avg_mae = np.mean(mae_scores)\n",
    "\n",
    "    # Append results to the DataFrame\n",
    "    results_df = results_df.append({'Model': model_name, 'RMSE': avg_rmse, 'MAE': avg_mae}, ignore_index=True)\n",
    "\n",
    "# Hyperparameter grid for NN\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(100,), (100, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'learning_rate_init': [0.001, 0.01],\n",
    "}\n",
    "\n",
    "# Initialise the NN Regressor model\n",
    "nn_model = MLPRegressor(max_iter=2000, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Perform Grid Search for NN\n",
    "grid_search = GridSearchCV(nn_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_scaled, y)\n",
    "best_nn_model = grid_search.best_estimator_\n",
    "\n",
    "# Compute RMSE and MAE for NN model and add it to the results DataFrame\n",
    "nn_rmse = np.sqrt(-grid_search.best_score_)\n",
    "nn_mae_scores = -cross_val_score(best_nn_model, X_scaled, y, cv=5, scoring='neg_mean_absolute_error')\n",
    "nn_avg_mae = np.mean(nn_mae_scores)\n",
    "\n",
    "# Append the NN results to the results DataFrame\n",
    "results_df = results_df.append({\n",
    "    'Model': 'Neural Network Regressor',\n",
    "    'RMSE': nn_rmse,\n",
    "    'MAE': nn_avg_mae\n",
    "}, ignore_index=True)\n",
    "\n",
    "# Print the updated results table\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1449e8e8",
   "metadata": {},
   "source": [
    "Evaluating model performance:The model with the lowest RMSE and MAE has the least prediction error on average. However, if a model is too complex, it might perform very well on the training set but poorly on unseen data (overfitting).\n",
    "\n",
    "Linear Regression: the most basic model and often serves as a benchmark. If complex models don't perform significantly better than linear regression, it could be a sign that the added complexity isn't beneficial.\n",
    "\n",
    "Random Forest & Gradient Boosting: These are ensemble models. In our results, they have much lower RMSE compared to the linear models, suggesting they are capturing nonlinear patterns in the data better.\n",
    "\n",
    "Ridge & Lasso: These are regularised linear regression models. The results show slightly higher error metrics than plain linear regression. This could indicate that the regularised versions aren't as beneficial for our dataset, or the hyperparameters need tuning.\n",
    "\n",
    "XGBoost: An advanced gradient boosting algorithm. It seems to be performing comparably to the Random Forest and Gradient Boosting models in our case.\n",
    "\n",
    "Neural Network Regressor: NNs are highly flexible models. Their performance can vary significantly with architecture and hyperparameters. Our NN is performing better than the linear models but not as good as the tree-based models. The NN is capturing more complex relationships in the dataset than simpler models like Linear Regression, Ridge, or Lasso. This could be due to non-linear patterns in the data which linear models can't capture as effectively. Even though the NN is flexible, in this specific instance, it hasn't outperformed tree-based models like Random Forest, Gradient Boosting, or XGBoost. It's also possible that the tree-based models are better suited for this dataset, or that the specific neural network architecture and hyperparameters chosen weren't optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9063fb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAEI 1km2 ID: 0.032061205274607665\n",
      "GRID_ExactCut_ID: 0.03633522462479362\n",
      "Easting_y: 0.03162297042149062\n",
      "Northing_y: 0.012987770141891427\n",
      "SO2: 0.062056202208094775\n",
      "NMVOC: 0.42571840196920224\n",
      "NH3: 0.02355037323216946\n",
      "CO: 0.04887203651307892\n",
      "CH4: 0.04969606085089379\n",
      "N2O: 0.02809915907548689\n",
      "Cd: 0.025181446314902604\n",
      "Hg: 0.012835127935211334\n",
      "Pb: 0.058198675432969596\n",
      "BaP: 0.050208850514976235\n",
      "PCB: 0.0317355005801904\n",
      "HCl: 4.34364484089935e-05\n",
      "PM10_cox_lag1: 0.03585803273884311\n",
      "PM10_cox_lag2: 0.03493952572278818\n"
     ]
    }
   ],
   "source": [
    "models[\"Random Forest\"].fit(X_train, y_train)\n",
    "\n",
    "feature_names = X.columns\n",
    "# Get feature importances\n",
    "importances = models[\"Random Forest\"].feature_importances_\n",
    "\n",
    "for feature, importance in zip(feature_names, importances):\n",
    "    print(f\"{feature}: {importance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820664b5",
   "metadata": {},
   "source": [
    "The importance value of each feature tells us how much that specific feature contributes to the model's decisions. The higher the importance, the more influential the feature is in determining the model's predictions.\n",
    "\n",
    "NMVOC has the highest feature importance at approximately 0.4257. This suggests that the Random Forest model considers NMVOC to be the most informative feature when making predictions.\n",
    "Other features with relatively high importances include SO2, CO, CH4, BaP, Pb. HCl has the smallest importance value, almost 0. This means it has very minimal influence in the model's decisions.\n",
    "However, feature importances in a Random Forest model don't tell you about the relationship direction (whether the relationship is positive or negative), but only the strength or magnitude of the influence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1192f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52275de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6311366570845751\n"
     ]
    }
   ],
   "source": [
    "# Modify the instantiation\n",
    "models[\"Random Forest\"] = RandomForestRegressor(oob_score=True, random_state=42)\n",
    "\n",
    "# Train the model (assuming X_train and y_train are your training data)\n",
    "models[\"Random Forest\"].fit(X_train, y_train)\n",
    "\n",
    "# Access the OOB score\n",
    "print(models[\"Random Forest\"].oob_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f6b1a8",
   "metadata": {},
   "source": [
    "The oob_score_ is the Out-Of-Bag (OOB) score for a Random Forest model. It's a way to measure the prediction accuracy of a Random Forest using the samples that were not included (left out) during the construction (bootstrap sampling) of individual trees. This means that the model's accuracy on the out-of-bag samples is approximately 63.11%. This is a pretty good score.\n",
    "It provides a way to get an estimate of the model's performance without needing a separate validation set, which can be especially valuable if you have limited data.\n",
    "It's a form of cross-validation that comes with the way Random Forests are constructed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8878ee9",
   "metadata": {},
   "source": [
    "Further evals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8499dc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/2430748696.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_results_df = val_results_df.append({'Model': model_name, 'RMSE': rmse_val, 'MAE': mae_val}, ignore_index=True)\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/2430748696.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_results_df = val_results_df.append({'Model': model_name, 'RMSE': rmse_val, 'MAE': mae_val}, ignore_index=True)\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/2430748696.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_results_df = val_results_df.append({'Model': model_name, 'RMSE': rmse_val, 'MAE': mae_val}, ignore_index=True)\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/2430748696.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_results_df = val_results_df.append({'Model': model_name, 'RMSE': rmse_val, 'MAE': mae_val}, ignore_index=True)\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/2430748696.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_results_df = val_results_df.append({'Model': model_name, 'RMSE': rmse_val, 'MAE': mae_val}, ignore_index=True)\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/2430748696.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_results_df = val_results_df.append({'Model': model_name, 'RMSE': rmse_val, 'MAE': mae_val}, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE for best model: 0.24275280057627383\n",
      "Test MAE for best model: 0.17751598684054395\n",
      "                      Model      RMSE       MAE\n",
      "0         Linear Regression  0.347094  0.274564\n",
      "1             Random Forest  0.263417  0.198446\n",
      "2         Gradient Boosting  0.265198  0.197990\n",
      "3                     Ridge  0.397263  0.324250\n",
      "4                     Lasso  0.396276  0.333569\n",
      "5                   XGBoost  0.263830  0.195503\n",
      "6  Neural Network Regressor  0.300518  0.228293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/2430748696.py:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_results_df = val_results_df.append({\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Splitting the data\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # 20% for testing\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 60% for training, 20% for validation\n",
    "\n",
    "# Initialise the models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n",
    "    \"Ridge\": Ridge(alpha=1.0, random_state=42),\n",
    "    \"Lasso\": Lasso(alpha=0.1, random_state=42),\n",
    "    \"XGBoost\": xgb.XGBRegressor(objective ='reg:squarederror', random_state=42)\n",
    "}\n",
    "\n",
    "# Results dataframe for validation\n",
    "val_results_df = pd.DataFrame(columns=['Model', 'RMSE', 'MAE'])\n",
    "\n",
    "# Train and validate the models\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    val_predictions = model.predict(X_val)\n",
    "    \n",
    "    rmse_val = np.sqrt(mean_squared_error(y_val, val_predictions))\n",
    "    mae_val = mean_absolute_error(y_val, val_predictions)\n",
    "\n",
    "    val_results_df = val_results_df.append({'Model': model_name, 'RMSE': rmse_val, 'MAE': mae_val}, ignore_index=True)\n",
    "\n",
    "# Scaling for NN\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Hyperparameter grid for NN\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(100,), (100, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'learning_rate_init': [0.001, 0.01],\n",
    "}\n",
    "\n",
    "# Initialise the NN Regressor model\n",
    "nn_model = MLPRegressor(max_iter=2000, random_state=42)\n",
    "\n",
    "# Grid Search for NN\n",
    "grid_search = GridSearchCV(nn_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "best_nn_model = grid_search.best_estimator_\n",
    "\n",
    "val_predictions_nn = best_nn_model.predict(X_val_scaled)\n",
    "rmse_val_nn = np.sqrt(mean_squared_error(y_val, val_predictions_nn))\n",
    "mae_val_nn = mean_absolute_error(y_val, val_predictions_nn)\n",
    "\n",
    "# Append the NN validation results\n",
    "val_results_df = val_results_df.append({\n",
    "    'Model': 'Neural Network Regressor',\n",
    "    'RMSE': rmse_val_nn,\n",
    "    'MAE': mae_val_nn\n",
    "}, ignore_index=True)\n",
    "\n",
    "# Final test evaluation\n",
    "# Choose best model based on validation RMSE/MAE \n",
    "\n",
    "best_model = models[\"Random Forest\"]\n",
    "test_predictions = best_model.predict(X_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "mae_test = mean_absolute_error(y_test, test_predictions)\n",
    "\n",
    "print(\"Test RMSE for best model:\", rmse_test)\n",
    "print(\"Test MAE for best model:\", mae_test)\n",
    "\n",
    "print(val_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdfb214",
   "metadata": {},
   "source": [
    "To identify 'best model' adding logic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d8984af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/2336692573.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_results_df = val_results_df.append({'Model': model_name, 'RMSE': rmse_val, 'MAE': mae_val}, ignore_index=True)\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/2336692573.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_results_df = val_results_df.append({'Model': model_name, 'RMSE': rmse_val, 'MAE': mae_val}, ignore_index=True)\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/2336692573.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_results_df = val_results_df.append({'Model': model_name, 'RMSE': rmse_val, 'MAE': mae_val}, ignore_index=True)\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/2336692573.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_results_df = val_results_df.append({'Model': model_name, 'RMSE': rmse_val, 'MAE': mae_val}, ignore_index=True)\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/2336692573.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_results_df = val_results_df.append({'Model': model_name, 'RMSE': rmse_val, 'MAE': mae_val}, ignore_index=True)\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/2336692573.py:36: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_results_df = val_results_df.append({'Model': model_name, 'RMSE': rmse_val, 'MAE': mae_val}, ignore_index=True)\n",
      "/var/folders/33/h5vv9bpj5pn1qwm0r096syj40000gn/T/ipykernel_76682/2336692573.py:64: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  val_results_df = val_results_df.append({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model based on Validation RMSE: Random Forest\n",
      "Test RMSE for best model: 0.24589050844471486\n",
      "Test MAE for best model: 0.18008911169668682\n",
      "                      Model      RMSE       MAE\n",
      "0         Linear Regression  0.347094  0.274564\n",
      "1             Random Forest  0.263417  0.198446\n",
      "2         Gradient Boosting  0.265198  0.197990\n",
      "3                     Ridge  0.397263  0.324250\n",
      "4                     Lasso  0.396276  0.333569\n",
      "5                   XGBoost  0.263830  0.195503\n",
      "6  Neural Network Regressor  0.300518  0.228293\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Splitting the data\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # 20% for testing\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)  # 60% for training, 20% for validation\n",
    "\n",
    "# Initialise the models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n",
    "    \"Ridge\": Ridge(alpha=1.0, random_state=42),\n",
    "    \"Lasso\": Lasso(alpha=0.1, random_state=42),\n",
    "    \"XGBoost\": xgb.XGBRegressor(objective ='reg:squarederror', random_state=42)\n",
    "}\n",
    "\n",
    "# Results dataframe for validation\n",
    "val_results_df = pd.DataFrame(columns=['Model', 'RMSE', 'MAE'])\n",
    "\n",
    "# Train and validate the models\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    val_predictions = model.predict(X_val)\n",
    "    \n",
    "    rmse_val = np.sqrt(mean_squared_error(y_val, val_predictions))\n",
    "    mae_val = mean_absolute_error(y_val, val_predictions)\n",
    "\n",
    "    val_results_df = val_results_df.append({'Model': model_name, 'RMSE': rmse_val, 'MAE': mae_val}, ignore_index=True)\n",
    "\n",
    "# Scaling for NN\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Hyperparameter grid for NN\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(100,), (100, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'learning_rate_init': [0.001, 0.01],\n",
    "}\n",
    "\n",
    "# Initialise the NN Regressor model\n",
    "nn_model = MLPRegressor(max_iter=2000, random_state=42)\n",
    "\n",
    "# Grid Search for NN\n",
    "grid_search = GridSearchCV(nn_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "best_nn_model = grid_search.best_estimator_\n",
    "\n",
    "val_predictions_nn = best_nn_model.predict(X_val_scaled)\n",
    "rmse_val_nn = np.sqrt(mean_squared_error(y_val, val_predictions_nn))\n",
    "mae_val_nn = mean_absolute_error(y_val, val_predictions_nn)\n",
    "\n",
    "# Append the NN validation results\n",
    "val_results_df = val_results_df.append({\n",
    "    'Model': 'Neural Network Regressor',\n",
    "    'RMSE': rmse_val_nn,\n",
    "    'MAE': mae_val_nn\n",
    "}, ignore_index=True)\n",
    "\n",
    "# After getting the validation results\n",
    "best_model_name = val_results_df.loc[val_results_df['RMSE'].idxmin()]['Model']\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "# Now, you can evaluate the best model on the test set:\n",
    "best_model.fit(X_temp, y_temp)  # First, we retrain on both training and validation combined\n",
    "test_predictions = best_model.predict(X_test)\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "mae_test = mean_absolute_error(y_test, test_predictions)\n",
    "\n",
    "print(f\"Best Model based on Validation RMSE: {best_model_name}\")\n",
    "print(\"Test RMSE for best model:\", rmse_test)\n",
    "print(\"Test MAE for best model:\", mae_test)\n",
    "\n",
    "print(val_results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1384d9c5",
   "metadata": {},
   "source": [
    "The Random Forest model has been identified as the best model based on the validation RMSE.\n",
    "On the test set, this Random Forest model achieved an RMSE of approximately 0.2459 and an MAE of approximately 0.1801.\n",
    "When comparing this test performance with its validation RMSE of 0.2634, the Random Forest model's performance is consistent and even slightly better on the test set, which is a good sign of generalisation.\n",
    "\n",
    "Comparing across all models:\n",
    "\n",
    "The Random Forest, XGBoost, and Gradient Boosting models are the top three performers in terms of RMSE. Their performance is closely matched, with Random Forest leading by a small margin in the validation results.\n",
    "The Neural Network Regressor, though not matching the tree-based models, still outperforms the linear models (Linear Regression, Ridge, and Lasso) in terms of RMSE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
