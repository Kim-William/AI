{"cells":[{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2202,"status":"ok","timestamp":1729617377222,"user":{"displayName":"김웅걸","userId":"04955122552547240370"},"user_tz":-120},"id":"DBPl837LuuGc","notebookRunGroups":{"groupValue":"1"},"outputId":"dda84084-24df-49fe-98aa-c6fa294f52da"},"outputs":[],"source":["import sys\n","sys.path.append('models')\n","\n","# from google.colab import drive\n","# drive.mount('/content/drive')\n","# sys.path.append('/content/drive/MyDrive/Dissertation/models')\n","# sys.path.append('/content/drive/MyDrive/Dissertation')"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":12698,"status":"ok","timestamp":1729617389918,"user":{"displayName":"김웅걸","userId":"04955122552547240370"},"user_tz":-120},"id":"i_OV5qsByAR9","outputId":"83d88c51-58fc-487e-c400-7f94331f6e30"},"outputs":[],"source":["# !pip install dask_ml\n","# !pip install scikeras\n","# import nltk\n","# nltk.download('stopwords')\n","# import nltk\n","# nltk.download('punkt')"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12703,"status":"ok","timestamp":1729617402618,"user":{"displayName":"김웅걸","userId":"04955122552547240370"},"user_tz":-120},"id":"Z6z3aV1muuGd","outputId":"ba7817cf-f945-4c0a-bc51-0cd9c7fa467d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Num GPUs Available:  1\n"]}],"source":["# importing libraries\n","import joblib\n","import os\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import classification_report, accuracy_score\n","import tensorflow as tf\n","from collections import defaultdict\n","import matplotlib.pyplot as plt\n","\n","# from models.bert import BERT\n","\n","# Load the TextPreprocessor class (assumed to be defined already)\n","from textpreprocessor import TextPreprocessor\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n","\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","if gpus:\n","    try:\n","        # Set memory growth\n","        for gpu in gpus:\n","            tf.config.experimental.set_memory_growth(gpu, True)\n","    except RuntimeError as e:\n","        print(e)"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1729617402618,"user":{"displayName":"김웅걸","userId":"04955122552547240370"},"user_tz":-120},"id":"wyNS08hbuuGe"},"outputs":[],"source":["NUM_SAMPLE = 10000\n","TEST_RATIO=0.2\n","BATCH_SIZE=32\n","EPOCHS = 5\n","MAX_WORD_COUNT = 5000\n","MAX_LENGTH = 100\n","OUTPUT_RESULT_DIR = \"Output/result\"\n","OUTPUT_MODELS_DIR = \"Output/models\"\n","USE_TEST_DATA = True\n","\n","os.makedirs(OUTPUT_RESULT_DIR, exist_ok=True)\n","os.makedirs(OUTPUT_MODELS_DIR, exist_ok=True)"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1729617402618,"user":{"displayName":"김웅걸","userId":"04955122552547240370"},"user_tz":-120},"id":"cIAG1a-CuuGe","notebookRunGroups":{"groupValue":"2"}},"outputs":[],"source":["# Define a function to plot training history\n","def plot_training_history(history, title=\"Model Training History\"):\n","    # Extract values from history\n","    acc = history.history['accuracy']\n","    val_acc = history.history['val_accuracy']\n","    loss = history.history['loss']\n","    val_loss = history.history['val_loss']\n","\n","    epochs = range(1, len(acc) + 1)\n","\n","    # Plot training and validation accuracy\n","    plt.figure(figsize=(14, 5))\n","\n","    # Accuracy plot\n","    plt.subplot(1, 2, 1)\n","    plt.plot(epochs, acc, 'b', label='Training Accuracy')\n","    plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n","    plt.title(f\"{title} - Accuracy\")\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","\n","    # Loss plot\n","    plt.subplot(1, 2, 2)\n","    plt.plot(epochs, loss, 'b', label='Training Loss')\n","    plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n","    plt.title(f\"{title} - Loss\")\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","# Function to plot training history from defaultdict data\n","def plot_training_history_from_dict(history, title=\"Model Training History\"):\n","    # Extract values from the dictionary\n","    acc = history['accuracy']\n","    val_acc = history['val_accuracy']\n","    loss = history['loss']\n","    val_loss = history['val_loss']\n","\n","    # Set up epoch range\n","    epochs = range(1, len(acc) + 1)\n","\n","    # Plot training and validation accuracy\n","    plt.figure(figsize=(14, 5))\n","\n","    # Accuracy plot\n","    plt.subplot(1, 2, 1)\n","    plt.plot(epochs, acc, 'b', label='Training Accuracy')\n","    plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n","    plt.title(f\"{title} - Accuracy\")\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","\n","    # Loss plot\n","    plt.subplot(1, 2, 2)\n","    plt.plot(epochs, loss, 'b', label='Training Loss')\n","    plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n","    plt.title(f\"{title} - Loss\")\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","# Dictionary to store the results\n","results = {\n","    'Model': [],\n","    'Training-Time':[],\n","    'Accuracy': [],\n","    'Precision (Class 0)': [],\n","    'Precision (Class 1)': [],\n","    'Recall (Class 0)': [],\n","    'Recall (Class 1)': [],\n","    'F1-Score (Class 0)': [],\n","    'F1-Score (Class 1)': []\n","}\n","\n","# Function to calculate accuracy and classification report\n","def _evaluate_model(training_time, model_name, y_test, y_pred):\n","    accuracy = accuracy_score(y_test, y_pred) * 100\n","    report = classification_report(y_test, y_pred, output_dict=True)\n","\n","    # Store the results\n","    results['Model'].append(model_name)\n","    results['Training-Time'].append(training_time)\n","    results['Accuracy'].append(accuracy)\n","    results['Precision (Class 0)'].append(report['0']['precision'])\n","    results['Precision (Class 1)'].append(report['1']['precision'])\n","    results['Recall (Class 0)'].append(report['0']['recall'])\n","    results['Recall (Class 1)'].append(report['1']['recall'])\n","    results['F1-Score (Class 0)'].append(report['0']['f1-score'])\n","    results['F1-Score (Class 1)'].append(report['1']['f1-score'])\n","\n","def _predict_model(model,X):\n","    y_pred_prob = model.predict(X)\n","    return [1 if prob > 0.5 else 0 for prob in y_pred_prob]\n","\n","def evaluate_model_class(model_class, X_test, y_test):\n","    y_pred = _predict_model(model_class.model, X_test)\n","    y_pred_random = _predict_model(model_class.random_search_cv.best_estimator_, X_test)\n","    y_pred_grid = _predict_model(model_class.grid_search_cv.best_estimator_, X_test)\n","    y_pred_best = _predict_model(model_class.best_model, X_test)\n","\n","    _evaluate_model(model_class.training_time, model_class.model_name, y_test, y_pred)\n","    _evaluate_model(model_class.random_search_time,  model_class.model_name + '_random_search', y_test, y_pred_random)\n","    _evaluate_model(model_class.grid_search_time,  model_class.model_name + '_grid_search', y_test, y_pred_grid)\n","    _evaluate_model(model_class.best_training_time,  model_class.model_name + '_best', y_test, y_pred_best)\n","\n","    df_results = pd.DataFrame(results)\n","    df_results.to_excel(os.path.join(OUTPUT_RESULT_DIR,'Model_Compare.xlsx'))\n","\n","def evaluate_xgboost_model_class(model_class, X_test, y_test):\n","    y_pred_xgb = _predict_model(model_class.model, model_class.convert_to_dmatrix(X_test, y_test))\n","    y_pred_xgb_random = _predict_model(model_class.random_search_cv.best_estimator_, X_test.toarray())\n","    y_pred_xgb_grid = _predict_model(model_class.grid_search_cv.best_estimator_, X_test.toarray())\n","    y_pred_xgb_best = _predict_model(model_class.best_model, model_class.convert_to_dmatrix(X_test, y_test))\n","\n","    # Call the function with your actual predictions (replace placeholders with your data)\n","    _evaluate_model(model_class.training_time, model_class.model_name, y_test, y_pred_xgb)\n","    _evaluate_model(model_class.random_search_time,  model_class.model_name + '_random_search', y_test, y_pred_xgb_random)\n","    _evaluate_model(model_class.grid_search_time,  model_class.model_name + '_grid_search', y_test, y_pred_xgb_grid)\n","    _evaluate_model(model_class.best_training_time,  model_class.model_name + '_best', y_test, y_pred_xgb_best)\n","\n","    df_results = pd.DataFrame(results)\n","    df_results.to_excel(os.path.join(OUTPUT_RESULT_DIR,'Model_Compare.xlsx'), index=False)\n","\n","def compare_models_accuracy_and_get_best_params(models, X_test, y_test):\n","    best_accuracy = 0\n","    best_params = None\n","    best_model_name = None\n","\n","    for model_name, model_class in models.items():\n","        # Get model's parameters (either from random search CV or from original model)\n","        if model_name!='Original':  # Check if it has random_search_cv\n","            y_pred_prob = model_class.predict(X_test)\n","            params = model_class.best_params_\n","        else:\n","            y_pred_prob = model_class.predict(X_test)\n","            params = model_class.get_params()\n","\n","        # Convert probabilities to binary predictions\n","        pred = [1 if prob > 0.5 else 0 for prob in y_pred_prob]\n","\n","        # Calculate accuracy\n","        accuracy = accuracy_score(y_test, pred) * 100\n","        print(f'{model_name} Accuracy: {accuracy}')\n","\n","        # Compare and keep track of the model with the highest accuracy\n","        if accuracy > best_accuracy:\n","            best_accuracy = accuracy\n","            best_params = params\n","            best_model_name = model_name\n","\n","    print(f'Best Model: {best_model_name} with Accuracy: {best_accuracy}')\n","    print(f'Best Parameters: {best_params}')\n","    return best_model_name, best_params\n"]},{"cell_type":"markdown","metadata":{"id":"V57jKpwbuuGe"},"source":["00. Text Pre-Processing"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":596811,"status":"ok","timestamp":1729617999426,"user":{"displayName":"김웅걸","userId":"04955122552547240370"},"user_tz":-120},"id":"sS7ho_aUuuGf","notebookRunGroups":{"groupValue":"1"},"outputId":"cdbf39a1-a42c-4130-a603-1a934d5fbf3f"},"outputs":[],"source":["# Initialize the Text Preprocessor\n","processor = TextPreprocessor(MAX_WORD_COUNT, MAX_LENGTH)\n","\n","if USE_TEST_DATA:\n","    INPUT_DIR = f\"Output/proto_models_rev2_{NUM_SAMPLE}\"\n","    # Load data\n","    df_train = pd.read_csv(os.path.join(INPUT_DIR, 'train_cleaned.csv'))\n","    df_test = pd.read_csv(os.path.join(INPUT_DIR, 'test_cleaned.csv'))\n","    # df_test = processor.load_data()\n","    X_train = df_train['review']\n","    X_test = df_test['review']\n","    y_train = df_train['polarity']\n","    y_test = df_test['polarity']\n","    # X_train_seq_padded = pickle.load(os.path.join(INPUT_DIR, 'X_train_pad.pkl'))\n","    # X_test_seq_padded = pickle.load(os.path.join(INPUT_DIR, 'X_test_pad.pkl'))\n","else:\n","    # Load data\n","    df_train, df_test = processor.parallel_load_data()\n","\n","    df_train_step1 = processor.remove_stopwords(df_train.copy())\n","    df_test_step1 = processor.remove_stopwords(df_test.copy())\n","\n","    print('----------TRAIN DATA----------')\n","    df_train_step2 = processor.filter_by_length_of_sentence(df_train_step1.copy(),50)\n","    print('----------TEST DATA----------')\n","    df_test_step2 = processor.filter_by_length_of_sentence(df_test_step1.copy(),50)\n","\n","    df_train_step3 = processor.sampling_data(df_train_step2, NUM_SAMPLE)\n","    df_test_step3 = processor.sampling_data(df_test_step2, int(NUM_SAMPLE*TEST_RATIO))\n","\n","    # Preprocess data\n","    df_train_step3 = processor.map_polarity(df_train_step3.copy())\n","    df_test_step3 = processor.map_polarity(df_test_step3.copy())\n","\n","    # Split data\n","    X_train, y_train = processor.split_data(df_train_step3)\n","    X_test, y_test = processor.split_data(df_test_step3)\n","    \n","    INPUT_DIR = f\"Output/proto_models_rev2_{NUM_SAMPLE}\"\n","    os.makedirs(INPUT_DIR, exist_ok=True)\n","    # Save data\n","    df_train_step3.to_csv(os.path.join(INPUT_DIR, 'train_cleaned.csv'), index=False)\n","    df_test_step3.to_csv(os.path.join(INPUT_DIR, 'test_cleaned.csv'), index=False)\n","\n","X_train_tfidf, X_test_tfidf = processor.vectorize_text(X_train, X_test)\n","X_train_pad, X_test_pad = processor.tokenization_and_padding(X_train, X_test)\n","\n","MAX_LENGTH = processor.max_length"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["# Importing necessary libraries\n","from transformers import BertTokenizer, TFBertForSequenceClassification, AdamWeightDecay, TFBertModel\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling1D\n","\n","from tensorflow.keras.models import Model\n","from keras_tuner import HyperParameters, RandomSearch\n","import tensorflow as tf\n","import numpy as np\n","import tensorflow\n","import time\n","\n","class BertLayer(tensorflow.keras.layers.Layer):\n","    def __init__(self, bert_model):\n","        super(BertLayer, self).__init__()\n","        self.bert_model = bert_model\n","    def call(self, inputs):\n","        input_ids, attention_mask = inputs\n","        bert_output = self.bert_model(input_ids=input_ids, attention_mask = attention_mask)\n","        bert_output = bert_output.last_hidden_state\n","        return bert_output\n","\n","class BERT():\n","    def __init__(self, max_length, epochs=15, batch_size=64, verbose=1):\n","        self.max_length = max_length\n","        self.epochs = epochs\n","        self.batch_size = batch_size\n","        self.verbose=verbose\n","        self.tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","    def __make_tokens(self, data):\n","        return self.tokenizer_bert(\n","            text=list(data),\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_tensors='tf',\n","            return_token_type_ids=False,\n","            return_attention_mask=True\n","        )\n","        \n","    # Convert to TensorFlow Tensor\n","    def create_tf_dataset(X, y, batch_size=32):\n","        input_ids = tf.convert_to_tensor(X[\"input_ids\"])\n","        attention_mask = tf.convert_to_tensor(X[\"attention_mask\"])\n","        y = tf.convert_to_tensor(y)\n","\n","        dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids\": input_ids, \"attention_mask\": attention_mask}, y))\n","        dataset = dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n","        return dataset\n","\n","    def train_model(self, X_train, y_train, X_test, y_test):\n","        X_train_tokens = self.__make_tokens(X_train)\n","        X_test_tokens = self.__make_tokens(X_test)\n","\n","        # Convert datasets\n","        train_dataset = self.create_tf_dataset(X_train_tokens, y_train)\n","        val_dataset = self.create_tf_dataset(X_test_tokens, y_test)\n","\n","        # Define a Keras-based model using TFBertModel\n","        input_ids = Input(shape=(self.max_length,), dtype='int32', name=\"input_ids\")\n","        attention_mask = Input(shape=(self.max_length,), dtype='int32', name=\"attention_mask\")\n","\n","\n","        bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n","        bert_model.trainable = False\n","        \n","        bert_layer  = BertLayer(bert_model=bert_model)\n","        bert_output=bert_layer([input_ids, attention_mask])\n","        pooled_output = GlobalAveragePooling1D()(bert_output.last_hidden_state)\n","\n","        dense = Dense(64, activation='relu')(pooled_output)\n","        dropout = Dropout(0.3)(dense)\n","        output = Dense(2, activation='softmax')(dropout)\n","\n","        self.model = Model(inputs=[input_ids, attention_mask], outputs=output)\n","\n","        # Adam optimizer with a learning rate of 2e-5\n","        optimizer = Adam(learning_rate=2e-5)\n","\n","        # Compile the model with a standard loss function\n","        loss_fn = SparseCategoricalCrossentropy(from_logits=False)\n","        self.model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n","\n","        # Adding EarlyStopping callback\n","        early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n","\n","        # Model summary\n","        self.model.summary()\n","\n","        # Train the model\n","        start_time = time.time()\n","        self.history = self.model.fit(\n","            {'input_ids': X_train_tokens['input_ids'], 'attention_mask': X_train_tokens['attention_mask']},\n","            y_train,\n","            validation_data=({'input_ids': X_test_tokens['input_ids'], 'attention_mask': X_test_tokens['attention_mask']}, y_test),\n","            epochs=self.epochs,\n","            batch_size=self.batch_size,\n","            callbacks=[early_stopping]  # Add EarlyStopping callback here\n","        )\n","\n","        self.training_time = time.time() - start_time"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["import keras\n","def build_bert_model():\n","    input_ids = Input(shape=(100,), dtype=tf.int32, name=\"input_ids\")\n","    attention_mask = Input(shape=(100,), dtype=tf.int32, name=\"attention_mask\")\n","\n","    class BertLayer(keras.layers.Layer):\n","        def __init__(self, bert_model):\n","            super(BertLayer, self).__init__()\n","            self.bert_model = bert_model\n","\n","        def call(self, inputs):\n","            input_ids, attention_mask = inputs\n","            bert_output= self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n","            bert_output = bert_output.last_hidden_state\n","            return bert_output\n","\n","    bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n","    bert_model.trainable = False\n","\n","    bert_layer = BertLayer(bert_model)\n","    bert_output = bert_layer([input_ids, attention_mask])\n","\n","    pooled_output = GlobalAveragePooling1D()(bert_output)\n","    dense = Dense(units=32, activation='relu')(pooled_output)\n","    output = Dense(2, activation='softmax')(dense)\n","\n","    model = Model(inputs=[input_ids, attention_mask], outputs=output)\n","\n","    learning_rate = 2e-5\n","    optimizer = Adam(learning_rate=learning_rate, weight_decay=0.01)\n","    model.compile(optimizer=optimizer, loss=SparseCategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n","\n","    return model"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["I0000 00:00:1729707118.993584  558596 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","I0000 00:00:1729707118.993633  558596 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","I0000 00:00:1729707118.993647  558596 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","I0000 00:00:1729707119.107929  558596 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","I0000 00:00:1729707119.107998  558596 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2024-10-23 20:11:59.108007: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n","I0000 00:00:1729707119.108041  558596 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n","Your kernel may have been built without NUMA support.\n","2024-10-23 20:11:59.108059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7537 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n"]}],"source":["\n","# Convert to TensorFlow Tensor\n","def create_tf_dataset(X, y, batch_size=32):\n","    input_ids = tf.convert_to_tensor(X[\"input_ids\"])\n","    attention_mask = tf.convert_to_tensor(X[\"attention_mask\"])\n","    y = tf.convert_to_tensor(y)\n","\n","    dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids\": input_ids, \"attention_mask\": attention_mask}, y))\n","    dataset = dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n","    return dataset\n","\n","tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","def __make_tokens(data):\n","    return tokenizer_bert(\n","        text=list(data),\n","        add_special_tokens=True,\n","        max_length=100,\n","        padding='max_length',\n","        truncation=True,\n","        return_tensors='tf',\n","        return_token_type_ids=False,\n","        return_attention_mask=True\n","    )\n","\n","X_train_tokens = __make_tokens(X_train)\n","X_test_tokens = __make_tokens(X_test)\n","\n","train_dataset = create_tf_dataset(X_train_tokens, y_train)\n","val_dataset = create_tf_dataset(X_test_tokens, y_test)\n"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]}],"source":["modelss = build_bert_model()"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"Unrecognized data type: x={'input_ids': <tf.Tensor: shape=(10000, 100), dtype=int32, numpy=\narray([[  101,  2128,  3676, ...,     0,     0,     0],\n       [  101,  3114,  6100, ...,     0,     0,     0],\n       [  101,  2204,  2143, ...,     0,     0,     0],\n       ...,\n       [  101,  4067,  2172, ...,     0,     0,     0],\n       [  101, 24005,  2081, ...,     0,     0,     0],\n       [  101,  9785, 17233, ...,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(10000, 100), dtype=int32, numpy=\narray([[1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       ...,\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>} (of type <class 'transformers.tokenization_utils_base.BatchEncoding'>)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodelss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/AI/Dissertation/.venv/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m~/AI/Dissertation/.venv/lib/python3.9/site-packages/keras/src/trainers/data_adapters/__init__.py:120\u001b[0m, in \u001b[0;36mget_data_adapter\u001b[0;34m(x, y, sample_weight, batch_size, steps_per_epoch, shuffle, class_weight)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GeneratorDataAdapter(x)\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# TODO: should we warn or not?\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# warnings.warn(\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m#     \"`shuffle=True` was passed, but will be ignored since the \"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized data type: x=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mValueError\u001b[0m: Unrecognized data type: x={'input_ids': <tf.Tensor: shape=(10000, 100), dtype=int32, numpy=\narray([[  101,  2128,  3676, ...,     0,     0,     0],\n       [  101,  3114,  6100, ...,     0,     0,     0],\n       [  101,  2204,  2143, ...,     0,     0,     0],\n       ...,\n       [  101,  4067,  2172, ...,     0,     0,     0],\n       [  101, 24005,  2081, ...,     0,     0,     0],\n       [  101,  9785, 17233, ...,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(10000, 100), dtype=int32, numpy=\narray([[1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       ...,\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>} (of type <class 'transformers.tokenization_utils_base.BatchEncoding'>)"]}],"source":["modelss.fit(X_train_tokens, y_test, epochs=3, callbacks=early_stopping)"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step\n","54.55\n"]}],"source":["# 예측 확률을 얻은 후, 가장 높은 확률을 가진 클래스 선택\n","y_pred_prob = modelss.predict(val_dataset)\n","\n","# 각 샘플에서 가장 높은 확률을 가진 클래스를 예측값으로 변환\n","y_pred = np.argmax(y_pred_prob, axis=1)\n","\n","# 정확도 계산\n","accuracy = accuracy_score(y_test, y_pred) * 100\n","print(accuracy)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# Function to calculate accuracy and classification report\n","def _evaluate_model(training_time, model_name, y_test, y_pred):\n","    accuracy = accuracy_score(y_test, y_pred) * 100\n","    report = classification_report(y_test, y_pred, output_dict=True)\n","\n","    # Store the results\n","    results['Model'].append(model_name)\n","    results['Training-Time'].append(training_time)\n","    results['Accuracy'].append(accuracy)\n","    results['Precision (Class 0)'].append(report['0']['precision'])\n","    results['Precision (Class 1)'].append(report['1']['precision'])\n","    results['Recall (Class 0)'].append(report['0']['recall'])\n","    results['Recall (Class 1)'].append(report['1']['recall'])\n","    results['F1-Score (Class 0)'].append(report['0']['f1-score'])\n","    results['F1-Score (Class 1)'].append(report['1']['f1-score'])\n","\n","def _predict_model(model,X):\n","    y_pred_prob = model.predict(X)\n","    return [1 if prob > 0.5 else 0 for prob in y_pred_prob]\n","\n","def evaluate_model_class(model_class, X_test, y_test):\n","    y_pred = _predict_model(model_class.model, X_test)\n","    y_pred_random = _predict_model(model_class.random_search_cv.best_estimator_, X_test)\n","    y_pred_grid = _predict_model(model_class.grid_search_cv.best_estimator_, X_test)\n","    y_pred_best = _predict_model(model_class.best_model, X_test)\n","\n","    _evaluate_model(model_class.training_time, model_class.model_name, y_test, y_pred)\n","    _evaluate_model(model_class.random_search_time,  model_class.model_name + '_random_search', y_test, y_pred_random)\n","    _evaluate_model(model_class.grid_search_time,  model_class.model_name + '_grid_search', y_test, y_pred_grid)\n","    _evaluate_model(model_class.best_training_time,  model_class.model_name + '_best', y_test, y_pred_best)\n","\n","    df_results = pd.DataFrame(results)\n","    df_results.to_excel(os.path.join(OUTPUT_RESULT_DIR,'Model_Compare.xlsx'))\n","\n","def evaluate_xgboost_model_class(model_class, X_test, y_test):\n","    y_pred_xgb = _predict_model(model_class.model, model_class.convert_to_dmatrix(X_test, y_test))\n","    y_pred_xgb_random = _predict_model(model_class.random_search_cv.best_estimator_, X_test.toarray())\n","    y_pred_xgb_grid = _predict_model(model_class.grid_search_cv.best_estimator_, X_test.toarray())\n","    y_pred_xgb_best = _predict_model(model_class.best_model, model_class.convert_to_dmatrix(X_test, y_test))\n","\n","    # Call the function with your actual predictions (replace placeholders with your data)\n","    _evaluate_model(model_class.training_time, model_class.model_name, y_test, y_pred_xgb)\n","    _evaluate_model(model_class.random_search_time,  model_class.model_name + '_random_search', y_test, y_pred_xgb_random)\n","    _evaluate_model(model_class.grid_search_time,  model_class.model_name + '_grid_search', y_test, y_pred_xgb_grid)\n","    _evaluate_model(model_class.best_training_time,  model_class.model_name + '_best', y_test, y_pred_xgb_best)\n","\n","    df_results = pd.DataFrame(results)\n","    df_results.to_excel(os.path.join(OUTPUT_RESULT_DIR,'Model_Compare.xlsx'), index=False)\n"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["\n","Search: Running Trial #1\n","\n","Value             |Best Value So Far |Hyperparameter\n","128               |128               |units\n","1e-05             |1e-05             |learning_rate\n","\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertModel were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","I0000 00:00:1729707047.432099  540404 service.cc:146] XLA service 0x7effbc003e80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","I0000 00:00:1729707047.432146  540404 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 3080, Compute Capability 8.6\n","2024-10-23 20:10:47.624372: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n","W0000 00:00:1729707047.635949  540404 assert_op.cc:38] Ignoring Assert operator functional_1/bert_layer_1/tf_bert_model/bert/embeddings/assert_less/Assert/Assert\n","2024-10-23 20:10:48.500567: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8906\n","2024-10-23 20:10:49.693770: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1', 16 bytes spill stores, 16 bytes spill loads\n","\n","2024-10-23 20:10:49.787514: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12042', 8 bytes spill stores, 8 bytes spill loads\n","\n","2024-10-23 20:10:49.809879: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1', 144 bytes spill stores, 144 bytes spill loads\n","\n","2024-10-23 20:10:49.854239: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12042', 32 bytes spill stores, 32 bytes spill loads\n","\n","2024-10-23 20:10:49.893401: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_11875', 148 bytes spill stores, 148 bytes spill loads\n","\n","2024-10-23 20:10:49.916448: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_12042', 32 bytes spill stores, 32 bytes spill loads\n","\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m  5/313\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 37ms/step - accuracy: 0.5926 - loss: 0.7025"]},{"name":"stderr","output_type":"stream","text":["I0000 00:00:1729707052.646527  540404 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[1m312/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5134 - loss: 0.6989"]},{"name":"stderr","output_type":"stream","text":["W0000 00:00:1729707063.862759  540405 assert_op.cc:38] Ignoring Assert operator functional_1/bert_layer_1/tf_bert_model/bert/embeddings/assert_less/Assert/Assert\n","2024-10-23 20:11:05.124263: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1', 16 bytes spill stores, 16 bytes spill loads\n","\n","2024-10-23 20:11:05.167559: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1', 36 bytes spill stores, 36 bytes spill loads\n","\n","2024-10-23 20:11:05.191736: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1', 144 bytes spill stores, 144 bytes spill loads\n","\n","2024-10-23 20:11:05.308919: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1', 204 bytes spill stores, 172 bytes spill loads\n","\n"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["# Initialize the BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Tokenizing the datasets\n","X_train_tokens = tokenizer(\n","    text=list(X_train),\n","    add_special_tokens=True,\n","    max_length=100,\n","    padding='max_length',\n","    truncation=True,\n","    return_tensors='tf',\n","    return_token_type_ids=False,\n","    return_attention_mask=True\n",")\n","\n","X_test_tokens = tokenizer(\n","    text=list(X_test),\n","    add_special_tokens=True,\n","    max_length=100,\n","    padding='max_length',\n","    truncation=True,\n","    return_tensors='tf',\n","    return_token_type_ids=False,\n","    return_attention_mask=True\n",")\n","\n","\n","# # Define BERT Model\n","# bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n","\n","# # Use Hugging Face's AdamWeightDecay optimizer\n","# optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n","# # Compile the model using a standard loss function\n","# loss_fn = SparseCategoricalCrossentropy(from_logits=True)\n","# bert_model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n","# # Model Summary\n","# bert_model.summary()\n","\n","# BERT Model Definition\n","def build_bert_model(hp):\n","    input_ids = Input(shape=(100,), dtype=tf.int32, name=\"input_ids\")\n","    attention_mask = Input(shape=(100,), dtype=tf.int32, name=\"attention_mask\")\n","\n","    class BertLayer(keras.layers.Layer):\n","        def __init__(self, bert_model):\n","            super(BertLayer, self).__init__()\n","            self.bert_model = bert_model\n","\n","        def call(self, inputs):\n","            input_ids, attention_mask = inputs\n","            bert_output= self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n","            bert_output = bert_output.last_hidden_state\n","            return bert_output\n","\n","    bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n","    bert_model.trainable = False\n","\n","    bert_layer = BertLayer(bert_model)\n","    bert_output = bert_layer([input_ids, attention_mask])\n","\n","    pooled_output = GlobalAveragePooling1D()(bert_output)\n","    dense = Dense(units=hp.Int(\"units\", min_value=32, max_value=128, step=32), activation='relu')(pooled_output)\n","    output = Dense(2, activation='softmax')(dense)\n","\n","    model = Model(inputs=[input_ids, attention_mask], outputs=output)\n","\n","    learning_rate = hp.Choice(\"learning_rate\", values=[1e-5, 2e-5, 3e-5])\n","    optimizer = Adam(learning_rate=learning_rate, weight_decay=0.01)\n","    model.compile(optimizer=optimizer, loss=SparseCategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n","\n","    return model\n","\n","# Convert to TensorFlow Tensor\n","def create_tf_dataset(X, y, batch_size=32):\n","    input_ids = tf.convert_to_tensor(X[\"input_ids\"])\n","    attention_mask = tf.convert_to_tensor(X[\"attention_mask\"])\n","    y = tf.convert_to_tensor(y)\n","\n","    dataset = tf.data.Dataset.from_tensor_slices(({\"input_ids\": input_ids, \"attention_mask\": attention_mask}, y))\n","    dataset = dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n","    return dataset\n","\n","# Convert datasets\n","train_dataset = create_tf_dataset(X_train_tokens, y_train)\n","val_dataset = create_tf_dataset(X_test_tokens, y_test)\n","\n","# Hyperparameter tuner\n","tuner = RandomSearch(\n","    build_bert_model,\n","    objective=\"val_accuracy\",\n","    max_trials=5,\n","    executions_per_trial=1,\n","    directory=\"bert_tuning\",\n","    project_name=\"bert_sentiment_analysis\"\n",")\n","\n","\n","# Perform the hyperparameter tuning search\n","tuner.search(\n","    train_dataset,\n","    validation_data=val_dataset,\n","    epochs=1\n",")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.20"}},"nbformat":4,"nbformat_minor":0}
