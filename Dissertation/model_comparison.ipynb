{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 16:27:36.153144: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-23 16:27:36.161595: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-23 16:27:36.164122: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-23 16:27:36.170646: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-23 16:27:36.550893: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1727101657.919596  274736 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1727101657.920311  274736 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1727101657.920335  274736 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "import time\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import tensorflow as tf\n",
    "\n",
    "from prototype.logstic_regression import Logistic_Regression\n",
    "from prototype.XGBoost import XGBoost\n",
    "from prototype.naive_bayes import Naive_Bayes\n",
    "from prototype.rnn import RNN\n",
    "from prototype.cnn import CNN\n",
    "from prototype.bert import BERT\n",
    "from prototype.bilstm import BiLSTM\n",
    "\n",
    "# Load the TextPreprocessor class (assumed to be defined already)\n",
    "from textpreprocessor import TextPreprocessor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLE = 10000\n",
    "TEST_RATIO=0.2\n",
    "BATCH_SIZE=32\n",
    "EPOCHS = 5\n",
    "MAX_WORD_COUNT = 5000\n",
    "MAX_LENGTH = 100\n",
    "OUTPUT_DIR = \"Output/proto_models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled Data Shape:\n",
      "(10000, 3)\n",
      "Sampled Data Shape:\n",
      "(2000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Text Preprocessor\n",
    "processor = TextPreprocessor(MAX_WORD_COUNT, MAX_LENGTH)\n",
    "\n",
    "# Load data\n",
    "df_train, df_test = processor.load_data(num_sample=NUM_SAMPLE, test_ratio=TEST_RATIO)\n",
    "\n",
    "# Preprocess data\n",
    "df_train = processor.preprocess(df_train)\n",
    "df_test = processor.preprocess(df_test)\n",
    "\n",
    "# Split data\n",
    "X_train, y_train = processor.split_data(df_train)\n",
    "X_test, y_test = processor.split_data(df_test)\n",
    "\n",
    "X_train_tfidf, X_test_tfidf = processor.vectorize_text(X_train, X_test)\n",
    "X_train_pad, X_test_pad = processor.tokenization_and_padding(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "01. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression = Logistic_Regression()\n",
    "logistic_model = logistic_regression.train_model(X_train_tfidf, y_train)\n",
    "y_pred_logistic = logistic_model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "02. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost = XGBoost()\n",
    "xgb_model = xgboost.train_model(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_prob_xgb = xgb_model.predict(xgboost.convert_to_dmatrix(X_test_tfidf, y_test))\n",
    "y_pred_xgb = [1 if prob > 0.5 else 0 for prob in y_pred_prob_xgb]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "03. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = Naive_Bayes()\n",
    "nb_model = naive_bayes.train_model(X_train_tfidf, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred_nb = nb_model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "04. Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1727101668.249242  274736 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1727101668.249338  274736 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1727101668.249359  274736 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1727101668.257179  274736 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1727101668.257281  274736 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-23 16:27:48.257289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1727101668.257317  274736 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-09-23 16:27:48.257348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7479 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1727101668.890449  275004 service.cc:146] XLA service 0x7fe48000a480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1727101668.890478  275004 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 3080, Compute Capability 8.6\n",
      "2024-09-23 16:27:48.908162: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-09-23 16:27:48.992334: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  7/313\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - accuracy: 0.5324 - loss: 0.6915"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1727101669.308641  275004 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 26ms/step - accuracy: 0.5833 - loss: 0.6512 - val_accuracy: 0.7875 - val_loss: 0.4625\n",
      "Epoch 2/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 27ms/step - accuracy: 0.8693 - loss: 0.3161 - val_accuracy: 0.7625 - val_loss: 0.5223\n",
      "Epoch 3/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 28ms/step - accuracy: 0.9598 - loss: 0.1249 - val_accuracy: 0.7790 - val_loss: 0.5855\n",
      "Epoch 4/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 28ms/step - accuracy: 0.9878 - loss: 0.0452 - val_accuracy: 0.7205 - val_loss: 0.8109\n",
      "Epoch 5/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 28ms/step - accuracy: 0.9901 - loss: 0.0299 - val_accuracy: 0.7310 - val_loss: 0.9411\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(processor.max_features, processor.max_length, EPOCHS, BATCH_SIZE)\n",
    "rnn_model = rnn.train_model(X_train_pad, y_train, X_test_pad, y_test)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_prob = rnn_model.predict(X_test_pad)\n",
    "y_pred_rnn = (y_pred_prob > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "05. Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6564 - loss: 0.6014 - val_accuracy: 0.8415 - val_loss: 0.3569\n",
      "Epoch 2/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9142 - loss: 0.2205 - val_accuracy: 0.8420 - val_loss: 0.3617\n",
      "Epoch 3/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9886 - loss: 0.0561 - val_accuracy: 0.8505 - val_loss: 0.4702\n",
      "Epoch 4/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9999 - loss: 0.0082 - val_accuracy: 0.8510 - val_loss: 0.5384\n",
      "Epoch 5/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.8500 - val_loss: 0.5776\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN(processor.max_features, processor.max_length, EPOCHS, BATCH_SIZE)\n",
    "cnn_model = cnn.train_model(X_train_pad, y_train, X_test_pad, y_test)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_prob = rnn_model.predict(X_test_pad)\n",
    "y_pred_cnn = (y_pred_prob > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "06. Bidirectional Encoder Representations from Transformers(BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0 (unused)\n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109483778 (417.65 MB)\n",
      "Trainable params: 109483778 (417.65 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "313/313 [==============================] - 88s 255ms/step - loss: 0.3044 - accuracy: 0.8696 - val_loss: 0.2210 - val_accuracy: 0.9105\n",
      "Epoch 2/5\n",
      "313/313 [==============================] - 79s 252ms/step - loss: 0.1445 - accuracy: 0.9510 - val_loss: 0.2168 - val_accuracy: 0.9210\n",
      "Epoch 3/5\n",
      "313/313 [==============================] - 78s 251ms/step - loss: 0.0810 - accuracy: 0.9756 - val_loss: 0.2727 - val_accuracy: 0.9100\n",
      "Epoch 4/5\n",
      "313/313 [==============================] - 79s 251ms/step - loss: 0.0460 - accuracy: 0.9871 - val_loss: 0.2919 - val_accuracy: 0.9240\n",
      "Epoch 5/5\n",
      "313/313 [==============================] - 79s 251ms/step - loss: 0.0354 - accuracy: 0.9908 - val_loss: 0.3243 - val_accuracy: 0.9205\n",
      "63/63 [==============================] - 6s 81ms/step\n"
     ]
    }
   ],
   "source": [
    "bert = BERT(processor.max_features, processor.max_length, EPOCHS, BATCH_SIZE)\n",
    "bert_model = bert.train_model(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_prob = bert_model.predict({'input_ids': bert.X_test_tokens['input_ids'], 'attention_mask': bert.X_test_tokens['attention_mask']}).logits\n",
    "y_pred_bert = np.argmax(y_pred_prob, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "07. Bidirectional Long Short-Term Memory (BiLSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - accuracy: 0.6264 - loss: 0.6304 - val_accuracy: 0.8070 - val_loss: 0.4262\n",
      "Epoch 2/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.8621 - loss: 0.3271 - val_accuracy: 0.8330 - val_loss: 0.3909\n",
      "Epoch 3/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.9140 - loss: 0.2187 - val_accuracy: 0.8295 - val_loss: 0.4609\n",
      "Epoch 4/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.9451 - loss: 0.1458 - val_accuracy: 0.8180 - val_loss: 0.5280\n",
      "Epoch 5/5\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - accuracy: 0.9718 - loss: 0.0886 - val_accuracy: 0.8045 - val_loss: 0.6203\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
     ]
    }
   ],
   "source": [
    "bilstm = BiLSTM(processor.tokenizer, EPOCHS, BATCH_SIZE)\n",
    "bilstm_model = bilstm.train_model(X_train_pad, y_train, X_test_pad,y_test)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_prob = bilstm_model.predict(X_test_pad)\n",
    "y_pred_bilstm = (y_pred_prob > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the results\n",
    "results = {\n",
    "    'Model': [],\n",
    "    'Training-Time':[],\n",
    "    'Accuracy': [],\n",
    "    'Precision (Class 0)': [],\n",
    "    'Precision (Class 1)': [],\n",
    "    'Recall (Class 0)': [],\n",
    "    'Recall (Class 1)': [],\n",
    "    'F1-Score (Class 0)': [],\n",
    "    'F1-Score (Class 1)': []\n",
    "}\n",
    "\n",
    "# Function to calculate accuracy and classification report\n",
    "def evaluate_model(model_class, y_test, y_pred):\n",
    "    accuracy = accuracy_score(y_test, y_pred) * 100\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # Store the results\n",
    "    results['Model'].append(model_class.model_name)\n",
    "    results['Training-Time'].append(model_class.training_time)\n",
    "    results['Accuracy'].append(accuracy)\n",
    "    results['Precision (Class 0)'].append(report['0']['precision'])\n",
    "    results['Precision (Class 1)'].append(report['1']['precision'])\n",
    "    results['Recall (Class 0)'].append(report['0']['recall'])\n",
    "    results['Recall (Class 1)'].append(report['1']['recall'])\n",
    "    results['F1-Score (Class 0)'].append(report['0']['f1-score'])\n",
    "    results['F1-Score (Class 1)'].append(report['1']['f1-score'])\n",
    "\n",
    "# Call the function with your actual predictions (replace placeholders with your data)\n",
    "evaluate_model(logistic_regression, y_test, y_pred_logistic)\n",
    "evaluate_model(xgboost, y_test, y_pred_xgb)\n",
    "evaluate_model(naive_bayes, y_test, y_pred_nb)\n",
    "evaluate_model(rnn, y_test, y_pred_rnn)\n",
    "evaluate_model(cnn, y_test, y_pred_cnn)\n",
    "evaluate_model(bert, y_test, y_pred_bert)\n",
    "evaluate_model(bilstm, y_test, y_pred_bilstm)\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Save result to an Excel file\n",
    "df_results.to_excel(os.path.join(OUTPUT_DIR, f'result(epoch{EPOCHS}_batch{BATCH_SIZE}_sample{NUM_SAMPLE}_ratio{TEST_RATIO}).xlsx'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Training-Time</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision (Class 0)</th>\n",
       "      <th>Precision (Class 1)</th>\n",
       "      <th>Recall (Class 0)</th>\n",
       "      <th>Recall (Class 1)</th>\n",
       "      <th>F1-Score (Class 0)</th>\n",
       "      <th>F1-Score (Class 1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic_Regression</td>\n",
       "      <td>0.025306</td>\n",
       "      <td>84.35</td>\n",
       "      <td>0.837233</td>\n",
       "      <td>0.849558</td>\n",
       "      <td>0.843238</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>0.840225</td>\n",
       "      <td>0.846644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>1.120255</td>\n",
       "      <td>82.40</td>\n",
       "      <td>0.823651</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.813525</td>\n",
       "      <td>0.833984</td>\n",
       "      <td>0.818557</td>\n",
       "      <td>0.829126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive_Bayes</td>\n",
       "      <td>0.002810</td>\n",
       "      <td>81.95</td>\n",
       "      <td>0.808425</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.825820</td>\n",
       "      <td>0.813477</td>\n",
       "      <td>0.817030</td>\n",
       "      <td>0.821904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RNN</td>\n",
       "      <td>44.071815</td>\n",
       "      <td>73.10</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.716192</td>\n",
       "      <td>0.673156</td>\n",
       "      <td>0.786133</td>\n",
       "      <td>0.709503</td>\n",
       "      <td>0.749534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNN</td>\n",
       "      <td>4.505993</td>\n",
       "      <td>73.10</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.716192</td>\n",
       "      <td>0.673156</td>\n",
       "      <td>0.786133</td>\n",
       "      <td>0.709503</td>\n",
       "      <td>0.749534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BERT</td>\n",
       "      <td>402.121258</td>\n",
       "      <td>92.05</td>\n",
       "      <td>0.904055</td>\n",
       "      <td>0.937310</td>\n",
       "      <td>0.936475</td>\n",
       "      <td>0.905273</td>\n",
       "      <td>0.919980</td>\n",
       "      <td>0.921013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BiLSTM</td>\n",
       "      <td>30.718415</td>\n",
       "      <td>80.45</td>\n",
       "      <td>0.803738</td>\n",
       "      <td>0.805207</td>\n",
       "      <td>0.793033</td>\n",
       "      <td>0.815430</td>\n",
       "      <td>0.798350</td>\n",
       "      <td>0.810286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Training-Time  Accuracy  Precision (Class 0)  \\\n",
       "0  Logistic_Regression       0.025306     84.35             0.837233   \n",
       "1              XGBoost       1.120255     82.40             0.823651   \n",
       "2          Naive_Bayes       0.002810     81.95             0.808425   \n",
       "3                  RNN      44.071815     73.10             0.750000   \n",
       "4                  CNN       4.505993     73.10             0.750000   \n",
       "5                 BERT     402.121258     92.05             0.904055   \n",
       "6               BiLSTM      30.718415     80.45             0.803738   \n",
       "\n",
       "   Precision (Class 1)  Recall (Class 0)  Recall (Class 1)  \\\n",
       "0             0.849558          0.843238          0.843750   \n",
       "1             0.824324          0.813525          0.833984   \n",
       "2             0.830508          0.825820          0.813477   \n",
       "3             0.716192          0.673156          0.786133   \n",
       "4             0.716192          0.673156          0.786133   \n",
       "5             0.937310          0.936475          0.905273   \n",
       "6             0.805207          0.793033          0.815430   \n",
       "\n",
       "   F1-Score (Class 0)  F1-Score (Class 1)  \n",
       "0            0.840225            0.846644  \n",
       "1            0.818557            0.829126  \n",
       "2            0.817030            0.821904  \n",
       "3            0.709503            0.749534  \n",
       "4            0.709503            0.749534  \n",
       "5            0.919980            0.921013  \n",
       "6            0.798350            0.810286  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Output/proto_models/bert_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Output/proto_models/bert_model/assets\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save Models\n",
    "joblib.dump(logistic_model, os.path.join(OUTPUT_DIR, 'logistic_regression_model.pkl'))\n",
    "joblib.dump(xgb_model, os.path.join(OUTPUT_DIR, 'xgboost_model.pkl'))\n",
    "joblib.dump(nb_model, os.path.join(OUTPUT_DIR, 'naive_bayes_model.pkl'))\n",
    "rnn_model.save(os.path.join(OUTPUT_DIR, 'rnn_model.h5'))  # or .tf\n",
    "cnn_model.save(os.path.join(OUTPUT_DIR, 'cnn_model.h5'))\n",
    "bert_model.save(os.path.join(OUTPUT_DIR, 'bert_model'), save_format='tf')\n",
    "bilstm_model.save(os.path.join(OUTPUT_DIR, 'bilstm_model.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Output/proto_models/bert_X_test_tokens.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(processor.tokenizer, os.path.join(OUTPUT_DIR, 'processor_tokenizer.pkl'))\n",
    "joblib.dump(bert.X_train_tokens, os.path.join(OUTPUT_DIR, 'bert_X_train_tokens.pkl'))\n",
    "joblib.dump(bert.X_test_tokens, os.path.join(OUTPUT_DIR, 'bert_X_test_tokens.pkl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
