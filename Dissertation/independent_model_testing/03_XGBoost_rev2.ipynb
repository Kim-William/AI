{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "import os\n",
    "import numpy as np\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "import joblib\n",
    "from dask.distributed import Client\n",
    "from dask_ml.model_selection import RandomizedSearchCV\n",
    "from dask_ml.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "OUTPUT_DIR = \"../Output/textpreprocess/240926/sample_step2_10000\"\n",
    "BEST_PARAM = os.path.join(OUTPUT_DIR, 'XGBoost_BestParam.pkl')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='xgboost')\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "X_train_tfidf = joblib.load(os.path.join(OUTPUT_DIR, 'X_train_tfidf.pkl'))\n",
    "X_test_tfidf = joblib.load(os.path.join(OUTPUT_DIR, 'X_test_tfidf.pkl'))\n",
    "y_train = joblib.load(os.path.join(OUTPUT_DIR, 'y_train.pkl'))\n",
    "y_test = joblib.load(os.path.join(OUTPUT_DIR, 'y_test.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DMatrix (XGBoost's data format)\n",
    "dtrain = xgb.DMatrix(X_train_tfidf, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test_tfidf, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 80.25%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.79      0.80       930\n",
      "           1       0.79      0.82      0.80       893\n",
      "\n",
      "    accuracy                           0.80      1823\n",
      "   macro avg       0.80      0.80      0.80      1823\n",
      "weighted avg       0.80      0.80      0.80      1823\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters for XGBoost\n",
    "params = {\n",
    "    'objective': 'binary:logistic',  # Binary classification\n",
    "    'max_depth': 6,\n",
    "    'eta': 0.3,\n",
    "    'verbosity': 1,\n",
    "    'eval_metric': 'logloss',\n",
    "    'tree_method': 'gpu_hist'  # Use GPU for training\n",
    "}\n",
    "\n",
    "# Train the model using GPU\n",
    "model = xgb.train(params, dtrain, num_boost_round=100)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_prob = model.predict(dtest)\n",
    "y_pred = [1 if prob > 0.5 else 0 for prob in y_pred_prob]\n",
    "\n",
    "# Model evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=5, threads_per_worker=2, memory_limit='10GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/dask_ml/model_selection/_search.py:1266: UserWarning: Sending large graph of size 694.59 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  futures = scheduler(\n",
      "/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/dask_ml/model_selection/_search.py:1316: UserWarning: Sending large graph of size 694.58 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  out = scheduler(dsk, keys, num_workers=n_jobs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'tree_method': 'hist', 'subsample': 0.75, 'n_estimators': 150, 'min_child_weight': 2, 'max_depth': 6, 'learning_rate': 0.16250000000000003, 'gamma': 0.15, 'device': 'cuda', 'colsample_bytree': 0.75}\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBClassifier(tree_method='gpu_hist', use_label_encoder=False, eval_metric='logloss', verbosity=0)\n",
    "\n",
    "# param_dist = {\n",
    "#     'learning_rate': np.linspace(0.01, 0.3, 10),  # 학습률 범위\n",
    "#     'n_estimators': np.arange(50, 300, 50),  # 트리 개수\n",
    "#     'max_depth': np.arange(3, 10, 1),  # 트리 최대 깊이\n",
    "#     'min_child_weight': np.arange(1, 6, 1),  # 최소 가중치\n",
    "#     'subsample': np.linspace(0.5, 1.0, 6),  # 샘플링 비율\n",
    "#     'colsample_bytree': np.linspace(0.5, 1.0, 6),  # 특성 샘플링 비율\n",
    "#     'gamma': np.linspace(0, 0.5, 5),  # 최소 손실 감소\n",
    "#     'tree_method': 'hist',  # GPU 사용,\n",
    "#     'device':'cuda'\n",
    "# }\n",
    "\n",
    "param_dist = {\n",
    "    'learning_rate': np.linspace(0.05, 0.2, 5),  # 범위를 좁게 설정\n",
    "    'n_estimators': np.arange(50, 200, 50),  # 범위를 줄임\n",
    "    'max_depth': np.arange(3, 8, 1),  # 트리 깊이 축소\n",
    "    'min_child_weight': np.arange(1, 4, 1),  # 줄임\n",
    "    'subsample': np.linspace(0.6, 0.9, 3),  # 샘플링 비율 축소\n",
    "    'colsample_bytree': np.linspace(0.6, 0.9, 3),  # 축소\n",
    "    'gamma': np.linspace(0, 0.3, 3),  # 축소\n",
    "    'tree_method': ['hist'],  # GPU 사용\n",
    "    'device': ['cuda'],\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV (Dask)\n",
    "random_search = RandomizedSearchCV(\n",
    "    model, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=50,  \n",
    "    scoring='accuracy', \n",
    "    cv=3,  \n",
    "    random_state=42,\n",
    "    n_jobs=-1,  \n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "random_search.fit(X_train_tfidf.toarray(), y_train)\n",
    "\n",
    "print(f\"Best parameters found: {random_search.best_params_}\")\n",
    "best_model = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 81.35%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.80      0.81       930\n",
      "           1       0.80      0.83      0.81       893\n",
      "\n",
      "    accuracy                           0.81      1823\n",
      "   macro avg       0.81      0.81      0.81      1823\n",
      "weighted avg       0.81      0.81      0.81      1823\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred_prob = random_search.predict(X_test_tfidf.toarray())\n",
    "y_pred = [1 if prob > 0.5 else 0 for prob in y_pred_prob]\n",
    "\n",
    "# Model evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at ../Output/textpreprocess/240926/sample_step2_10000/XGBoost_BestParam.pkl\n"
     ]
    }
   ],
   "source": [
    "joblib.dump(random_search.best_params_, BEST_PARAM)\n",
    "\n",
    "print(f\"Best model saved at {BEST_PARAM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model loaded from ../Output/textpreprocess/240926/sample_step2_10000/XGBoost_BestParam.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load the saved best model\n",
    "best_param = joblib.load(BEST_PARAM)\n",
    "\n",
    "print(f\"Best model loaded from {BEST_PARAM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/dask_ml/model_selection/_search.py:1266: UserWarning: Sending large graph of size 694.77 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  futures = scheduler(\n",
      "/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/dask_ml/model_selection/_search.py:1316: UserWarning: Sending large graph of size 694.58 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider scattering data ahead of time and using futures.\n",
      "  out = scheduler(dsk, keys, num_workers=n_jobs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found by GridSearchCV: {'colsample_bytree': 0.7, 'device': 'cuda', 'gamma': 0.2, 'learning_rate': 0.15250000000000002, 'max_depth': 6, 'min_child_weight': 2, 'n_estimators': 200, 'subsample': 0.8, 'tree_method': 'hist'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the XGBoost classifier with base settings\n",
    "model = xgb.XGBClassifier(\n",
    "    tree_method='gpu_hist',  # Use GPU for histogram optimization\n",
    "    use_label_encoder=False,  # Disable label encoder warning\n",
    "    eval_metric='logloss',  # Use log loss as the evaluation metric\n",
    "    verbosity=0  # Suppress unnecessary warnings and logs\n",
    ")\n",
    "\n",
    "# Assuming you have the best parameters from RandomizedSearchCV saved as `random_search.best_params_`\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Dynamically create the parameter grid based on the best parameters found by RandomizedSearchCV\n",
    "param_grid = {\n",
    "    'learning_rate': np.linspace(best_params['learning_rate'] - 0.01, best_params['learning_rate'] + 0.01, 3),  \n",
    "    'n_estimators': [best_params['n_estimators'] - 50, best_params['n_estimators'], best_params['n_estimators'] + 50],  \n",
    "    'max_depth': [best_params['max_depth'] - 1, best_params['max_depth'], best_params['max_depth'] + 1],  \n",
    "    'min_child_weight': [best_params['min_child_weight'] - 1, best_params['min_child_weight']],  \n",
    "    'subsample': np.linspace(best_params['subsample'] - 0.05, best_params['subsample'] + 0.05, 3),  \n",
    "    'colsample_bytree': np.linspace(best_params['colsample_bytree'] - 0.05, best_params['colsample_bytree'] + 0.05, 3),  \n",
    "    'gamma': np.linspace(best_params['gamma'] - 0.05, best_params['gamma'] + 0.05, 3),  \n",
    "    'tree_method': ['hist'],  # Use GPU\n",
    "    'device': ['cuda'],  # GPU usage\n",
    "}\n",
    "\n",
    "# Proceed with GridSearchCV using this dynamically generated param_grid\n",
    "grid_search = GridSearchCV(\n",
    "    model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train_tfidf.toarray(), y_train)\n",
    "\n",
    "# Print the best parameters and model\n",
    "print(f\"Best parameters found by GridSearchCV: {grid_search.best_params_}\")\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 81.35%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.80      0.81       930\n",
      "           1       0.80      0.83      0.81       893\n",
      "\n",
      "    accuracy                           0.81      1823\n",
      "   macro avg       0.81      0.81      0.81      1823\n",
      "weighted avg       0.81      0.81      0.81      1823\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred_prob = grid_search.predict(X_test_tfidf.toarray())\n",
    "y_pred = [1 if prob > 0.5 else 0 for prob in y_pred_prob]\n",
    "\n",
    "# Model evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found by GridSearchCV: {'colsample_bytree': 0.7, 'device': 'cuda', 'gamma': 0.2, 'learning_rate': 0.15250000000000002, 'max_depth': 6, 'min_child_weight': 2, 'n_estimators': 200, 'subsample': 0.8, 'tree_method': 'hist'}\n",
      "Best model saved at ../Output/textpreprocess/240926/sample_step2_10000/XGBoost_best_model_grid_search.pkl\n",
      "Best parameters saved at ../Output/textpreprocess/240926/sample_step2_10000/XGBoost_best_params_grid_search.pkl\n"
     ]
    }
   ],
   "source": [
    "# Get the best parameters and estimator\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(f\"Best parameters found by GridSearchCV: {best_params}\")\n",
    "\n",
    "\n",
    "model_save_path = os.path.join(OUTPUT_DIR, \"XGBoost_best_model_grid_search.pkl\")\n",
    "params_save_path = os.path.join(OUTPUT_DIR, \"XGBoost_best_params_grid_search.pkl\")\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, model_save_path)\n",
    "print(f\"Best model saved at {model_save_path}\")\n",
    "\n",
    "# Save the best parameters\n",
    "joblib.dump(best_params, params_save_path)\n",
    "print(f\"Best parameters saved at {params_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
