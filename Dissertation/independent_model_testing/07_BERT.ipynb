{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Libs for Randomsearch]\n",
    "TensorFlow version: 2.11.0\n",
    "Transformers version: 4.17.0\n",
    "Keras version: 2.11.0\n",
    "[Libs for Bert]\n",
    "TensorFlow version: 2.17.0\n",
    "Transformers version: 4.45.2\n",
    "Keras version: 3.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-13 21:25:22.114580: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-13 21:25:22.527848: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu:/usr/local/cuda-12.3/lib64:/usr/local/cuda-12.3/lib64:/usr/local/cuda-11.8/lib64\n",
      "2024-10-13 21:25:22.527894: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib/x86_64-linux-gnu:/usr/local/cuda-12.3/lib64:/usr/local/cuda-12.3/lib64:/usr/local/cuda-11.8/lib64\n",
      "2024-10-13 21:25:22.527897: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.11.0\n",
      "Transformers version: 4.45.2\n",
      "Keras version: 2.11.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import transformers\n",
    "import keras\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-13 21:25:24.174450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-13 21:25:24.184371: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-13 21:25:24.184401: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification, AdamWeightDecay, TFBertModel\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling1D\n",
    "\n",
    "from tensorflow.keras.utils import custom_object_scope\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from keras_tuner import HyperParameters, RandomSearch\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "INPUT_DIR = \"../Output/proto_models_rev2\"\n",
    "# Load data\n",
    "df_train = pd.read_csv(os.path.join(INPUT_DIR, 'train_cleaned.csv'))\n",
    "df_test = pd.read_csv(os.path.join(INPUT_DIR, 'test_cleaned.csv'))\n",
    "# df_test = processor.load_data()\n",
    "X_train = df_train['review']\n",
    "X_test = df_test['review']\n",
    "y_train = df_train['polarity']\n",
    "y_test = df_test['polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-13 21:25:30.111673: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-13 21:25:30.116601: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-13 21:25:30.116631: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-13 21:25:30.116643: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-13 21:25:30.245322: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-13 21:25:30.245362: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-13 21:25:30.245366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-10-13 21:25:30.245382: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-13 21:25:30.245403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7537 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenizing the datasets\n",
    "X_train_tokens = tokenizer(\n",
    "    text=list(X_train),\n",
    "    add_special_tokens=True,\n",
    "    max_length=100,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True\n",
    ")\n",
    "\n",
    "X_test_tokens = tokenizer(\n",
    "    text=list(X_test),\n",
    "    add_special_tokens=True,\n",
    "    max_length=100,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      " dropout_75 (Dropout)        multiple                  0 (unused)\n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109483778 (417.65 MB)\n",
      "Trainable params: 109483778 (417.65 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "313/313 [==============================] - 87s 261ms/step - loss: 0.4170 - accuracy: 0.8066 - val_loss: 0.3438 - val_accuracy: 0.8455\n",
      "Epoch 2/3\n",
      "313/313 [==============================] - 80s 257ms/step - loss: 0.2485 - accuracy: 0.9030 - val_loss: 0.3148 - val_accuracy: 0.8670\n",
      "Epoch 3/3\n",
      "313/313 [==============================] - 81s 257ms/step - loss: 0.1361 - accuracy: 0.9529 - val_loss: 0.3355 - val_accuracy: 0.8750\n",
      "63/63 [==============================] - 6s 85ms/step\n"
     ]
    }
   ],
   "source": [
    "# Define BERT Model\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Use Hugging Face's AdamWeightDecay optimizer\n",
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n",
    "# Compile the model using a standard loss function\n",
    "loss_fn = SparseCategoricalCrossentropy(from_logits=True)\n",
    "bert_model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "# Model Summary\n",
    "bert_model.summary()\n",
    "\n",
    "# Train the model\n",
    "hist = bert_model.fit(\n",
    "    {'input_ids': X_train_tokens['input_ids'], 'attention_mask': X_train_tokens['attention_mask']},\n",
    "    y_train,\n",
    "    validation_data=({'input_ids': X_test_tokens['input_ids'], 'attention_mask': X_test_tokens['attention_mask']}, y_test),\n",
    "    epochs=3,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_logits = bert_model.predict({'input_ids': X_test_tokens['input_ids'], 'attention_mask': X_test_tokens['attention_mask']}).logits\n",
    "y_pred = np.argmax(y_pred_logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 87.50%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86       893\n",
      "           1       0.88      0.89      0.89      1107\n",
      "\n",
      "    accuracy                           0.88      2000\n",
      "   macro avg       0.87      0.87      0.87      2000\n",
      "weighted avg       0.87      0.88      0.87      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Tensorflow version 2.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 11 Complete [00h 01m 36s]\n",
      "val_accuracy: 0.8585000038146973\n",
      "\n",
      "Best val_accuracy So Far: 0.8694999814033508\n",
      "Total elapsed time: 05h 34m 29s\n"
     ]
    }
   ],
   "source": [
    "def build_bert_model(hp):\n",
    "    input_ids = Input(shape=(100,), dtype='int32', name=\"input_ids\")\n",
    "    attention_mask = Input(shape=(100,), dtype='int32', name=\"attention_mask\")\n",
    "    \n",
    "    bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    bert_output = bert_model(input_ids, attention_mask=attention_mask)[0] \n",
    "    pooled_output = GlobalAveragePooling1D()(bert_output)  \n",
    "    \n",
    "    dense = Dense(units=hp.Int(\"units\", min_value=32, max_value=128, step=16), activation='relu')(pooled_output)\n",
    "    output = Dense(2, activation='softmax')(dense)\n",
    "    \n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "    \n",
    "    learning_rate = hp.Choice(\"learning_rate\", values=[1e-5, 2e-5, 3e-5])\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=SparseCategoricalCrossentropy(), metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_bert_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=20,\n",
    "    executions_per_trial=1,\n",
    "    directory=\"bert_tuning\",\n",
    "    project_name=\"bert_sentiment_analysis\"\n",
    ")\n",
    "\n",
    "train_data = (\n",
    "    {\"input_ids\": X_train_tokens[\"input_ids\"], \"attention_mask\": X_train_tokens[\"attention_mask\"]},\n",
    "    y_train\n",
    ")\n",
    "val_data = (\n",
    "    {\"input_ids\": X_test_tokens[\"input_ids\"], \"attention_mask\": X_test_tokens[\"attention_mask\"]},\n",
    "    y_test\n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    x=train_data[0],\n",
    "    y=train_data[1],\n",
    "    validation_data=val_data,\n",
    "    epochs=1,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "Learning Rate: 2e-05\n",
      "63/63 [==============================] - 7s 88ms/step - loss: 0.4337 - accuracy: 0.8695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.433656245470047, 0.8694999814033508]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the best model and hyperparameters\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(f\"Learning Rate: {best_hyperparameters.get('learning_rate')}\")\n",
    "# print(f\"Weight Decay: {best_hyperparameters.get('weight_decay')}\")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model.evaluate(\n",
    "    {'input_ids': X_test_tokens['input_ids'], 'attention_mask': X_test_tokens['attention_mask']},\n",
    "    y_test\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 6s 85ms/step\n",
      "Model Accuracy: 86.95%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.83      0.85       893\n",
      "           1       0.87      0.90      0.88      1107\n",
      "\n",
      "    accuracy                           0.87      2000\n",
      "   macro avg       0.87      0.87      0.87      2000\n",
      "weighted avg       0.87      0.87      0.87      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict with the best model\n",
    "y_pred_logits = best_model.predict({'input_ids': X_test_tokens['input_ids'], 'attention_mask': X_test_tokens['attention_mask']})\n",
    "y_pred = np.argmax(y_pred_logits, axis=-1)\n",
    "# Model evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = os.path.join(INPUT_DIR, 'bert_best_model.h5')\n",
    "params_filename = os.path.join(INPUT_DIR,'bert_best_params.pkl')\n",
    "\n",
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "# Save the best hyperparameters and model after tuning\n",
    "def save_model_and_params(best_model, best_params, model_filename=model_filename, params_filename=params_filename):\n",
    "    # Save best model\n",
    "    best_model.save(model_filename)\n",
    "        \n",
    "    # Save best hyperparameters\n",
    "    with open(params_filename, 'wb') as params_file:\n",
    "        pickle.dump(best_params, params_file)\n",
    "\n",
    "# Example usage (after tuning)\n",
    "save_model_and_params(best_model, best_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = os.path.join(INPUT_DIR, 'bert_best_model.h5')\n",
    "params_filename = os.path.join(INPUT_DIR,'bert_best_params.pkl')\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the best hyperparameters and model from the saved files\n",
    "def load_model_and_params(model_filename=model_filename, params_filename=params_filename):\n",
    "    # Load best hyperparameters\n",
    "    with open(params_filename, 'rb') as params_file:\n",
    "        best_params = pickle.load(params_file)\n",
    "\n",
    "    # Load best model using custom object scope for TFBertModel\n",
    "    with custom_object_scope({'TFBertModel': TFBertModel}):\n",
    "        best_model = load_model(model_filename)\n",
    "            \n",
    "\n",
    "    return  best_model, best_params\n",
    "\n",
    "# Example usage\n",
    "best_model, best_params = load_model_and_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_2 (TFBertModel)  TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 100,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1 (Gl  (None, 768)         0           ['tf_bert_model_2[0][0]']        \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 68)           52292       ['global_average_pooling1d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 2)            138         ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,534,670\n",
      "Trainable params: 109,534,670\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build a new BERT model with the loaded hyperparameters\n",
    "def build_bert_model_with_params(best_params):\n",
    "\n",
    "    input_ids = Input(shape=(100,), dtype='int32', name=\"input_ids\")\n",
    "    attention_mask = Input(shape=(100,), dtype='int32', name=\"attention_mask\")\n",
    "    \n",
    "    bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    bert_output = bert_model(input_ids, attention_mask=attention_mask)[0] \n",
    "    pooled_output = GlobalAveragePooling1D()(bert_output)  \n",
    "    \n",
    "    dense = Dense(units=68, activation='relu')(pooled_output)\n",
    "    output = Dense(2, activation='softmax')(dense)\n",
    "    \n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "    # # Create a new BERT model with the best hyperparameters\n",
    "    # bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Compile the model using the best hyperparameters\n",
    "    learning_rate = best_params.get('learning_rate')  # Default value if not found\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Build a new model with the best hyperparameters\n",
    "best_model = build_bert_model_with_params(best_params)\n",
    "\n",
    "# Print the model summary\n",
    "best_model.summary()\n",
    "\n",
    "model_filename = os.path.join(INPUT_DIR, 'bert_best_model.h5')\n",
    "best_model.save(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "313/313 [==============================] - 90s 243ms/step - loss: 0.4042 - accuracy: 0.4703 - val_loss: 0.3152 - val_accuracy: 0.4715\n",
      "Epoch 2/2\n",
      "313/313 [==============================] - 75s 238ms/step - loss: 0.2347 - accuracy: 0.4707 - val_loss: 0.3232 - val_accuracy: 0.4710\n"
     ]
    }
   ],
   "source": [
    "# Train the loaded model with the same dataset\n",
    "history = best_model.fit(\n",
    "    {'input_ids': X_train_tokens['input_ids'], 'attention_mask': X_train_tokens['attention_mask']},\n",
    "    y_train,\n",
    "    validation_data=({'input_ids': X_test_tokens['input_ids'], 'attention_mask': X_test_tokens['attention_mask']}, y_test),\n",
    "    epochs=2,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 7s 85ms/step\n",
      "Model Accuracy: 86.05%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85       893\n",
      "           1       0.89      0.85      0.87      1107\n",
      "\n",
      "    accuracy                           0.86      2000\n",
      "   macro avg       0.86      0.86      0.86      2000\n",
      "weighted avg       0.86      0.86      0.86      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict with the best model\n",
    "y_pred_logits = best_model.predict({'input_ids': X_test_tokens['input_ids'], 'attention_mask': X_test_tokens['attention_mask']})\n",
    "y_pred = np.argmax(y_pred_logits, axis=-1)\n",
    "# Model evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a function to plot training history\n",
    "def plot_training_history(history, title=\"Model Training History\"):\n",
    "    # Extract values from history\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    \n",
    "    # Plot training and validation accuracy\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, 'b', label='Training Accuracy')\n",
    "    plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
    "    plt.title(f\"{title} - Accuracy\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, 'b', label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "    plt.title(f\"{title} - Loss\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for hist\n",
    "plot_training_history(hist, title=\"Initial Model Training History\")\n",
    "\n",
    "# Plot for history\n",
    "plot_training_history(history, title=\"Searched Model Training History\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
