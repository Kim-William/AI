{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Libs for Randomsearch]\n",
    "TensorFlow version: 2.11.0\n",
    "Transformers version: 4.17.0\n",
    "Keras version: 2.11.0\n",
    "[Libs for Bert]\n",
    "TensorFlow version: 2.17.0\n",
    "Transformers version: 4.45.2\n",
    "Keras version: 3.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-19 21:44:11.010215: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-19 21:44:11.019071: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-19 21:44:11.021548: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-19 21:44:11.028192: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-19 21:44:11.812237: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.17.0\n",
      "Transformers version: 4.45.2\n",
      "Keras version: 3.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import transformers\n",
    "import keras\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification, AdamWeightDecay, TFBertModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_337524/2140621882.py:11: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner.tuners import RandomSearch\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1729367054.335796  337524 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1729367054.336943  337524 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1729367054.336967  337524 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling1D\n",
    "\n",
    "from tensorflow.keras.utils import custom_object_scope\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "# from keras_tuner import HyperParameters, RandomSearch\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "INPUT_DIR = \"../Output/proto_models_rev2_1000\"\n",
    "# Load data\n",
    "df_train = pd.read_csv(os.path.join(INPUT_DIR, 'train_cleaned.csv'))\n",
    "df_test = pd.read_csv(os.path.join(INPUT_DIR, 'test_cleaned.csv'))\n",
    "# df_test = processor.load_data()\n",
    "X_train = df_train['review']\n",
    "X_test = df_test['review']\n",
    "y_train = df_train['polarity']\n",
    "y_test = df_test['polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1729367055.859332  337524 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1729367055.859381  337524 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1729367055.859395  337524 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1729367055.945629  337524 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1729367055.945685  337524 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-19 21:44:15.945692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1729367055.945714  337524 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-10-19 21:44:15.945728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7537 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenizing the datasets\n",
    "X_train_tokens = tokenizer(\n",
    "    text=list(X_train),\n",
    "    add_special_tokens=True,\n",
    "    max_length=50,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True\n",
    ")\n",
    "\n",
    "X_test_tokens = tokenizer(\n",
    "    text=list(X_test),\n",
    "    add_special_tokens=True,\n",
    "    max_length=50,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0 (unused)\n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109483778 (417.65 MB)\n",
      "Trainable params: 109483778 (417.65 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "32/32 [==============================] - 18s 305ms/step - loss: 0.6653 - accuracy: 0.5990 - val_loss: 0.5260 - val_accuracy: 0.7800\n",
      "Epoch 2/3\n",
      "32/32 [==============================] - 9s 279ms/step - loss: 0.4539 - accuracy: 0.8100 - val_loss: 0.3039 - val_accuracy: 0.8950\n",
      "Epoch 3/3\n",
      "32/32 [==============================] - 9s 271ms/step - loss: 0.2748 - accuracy: 0.9070 - val_loss: 0.1719 - val_accuracy: 0.9500\n",
      "7/7 [==============================] - 2s 83ms/step\n"
     ]
    }
   ],
   "source": [
    "# Define BERT Model\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Use Hugging Face's AdamWeightDecay optimizer\n",
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n",
    "# Compile the model using a standard loss function\n",
    "loss_fn = SparseCategoricalCrossentropy(from_logits=True)\n",
    "bert_model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "# Model Summary\n",
    "bert_model.summary()\n",
    "\n",
    "# Train the model\n",
    "hist = bert_model.fit(\n",
    "    {'input_ids': X_train_tokens['input_ids'], 'attention_mask': X_train_tokens['attention_mask']},\n",
    "    y_train,\n",
    "    validation_data=({'input_ids': X_test_tokens['input_ids'], 'attention_mask': X_test_tokens['attention_mask']}, y_test),\n",
    "    epochs=3,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_logits = bert_model.predict({'input_ids': X_test_tokens['input_ids'], 'attention_mask': X_test_tokens['attention_mask']}).logits\n",
    "y_pred = np.argmax(y_pred_logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 95.00%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.94      0.95       102\n",
      "           1       0.94      0.96      0.95        98\n",
      "\n",
      "    accuracy                           0.95       200\n",
      "   macro avg       0.95      0.95      0.95       200\n",
      "weighted avg       0.95      0.95      0.95       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Tensorflow version 2.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from bert_tuning/bert_sentiment_analysis/tuner0.json\n",
      "input_ids <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "attention_mask <class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/keras/src/ops/nn.py:545: UserWarning: You are using a softmax over axis -1 of a tensor of shape (1000, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All `inputs` values must be KerasTensors. Received: inputs=[<tf.Tensor: shape=(1000, 50), dtype=int32, numpy=\narray([[  101, 23331,  2190, ...,     0,     0,     0],\n       [  101,  2028,  2190, ...,  1012,  2065,   102],\n       [  101, 15736,  2080, ...,  2909,  3726,   102],\n       ...,\n       [  101,  2625,  2093, ...,     0,     0,     0],\n       [  101,  4149,  2047, ...,     0,     0,     0],\n       [  101,  2299,  4276, ...,  4602,  4978,   102]], dtype=int32)>, <tf.Tensor: shape=(1000, 50), dtype=int32, numpy=\narray([[1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 1, 1, 1],\n       [1, 1, 1, ..., 1, 1, 1],\n       ...,\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 1, 1, 1]], dtype=int32)>] including invalid value [[  101 23331  2190 ...     0     0     0]\n [  101  2028  2190 ...  1012  2065   102]\n [  101 15736  2080 ...  2909  3726   102]\n ...\n [  101  2625  2093 ...     0     0     0]\n [  101  4149  2047 ...     0     0     0]\n [  101  2299  4276 ...  4602  4978   102]] of type <class 'tensorflow.python.framework.ops.EagerTensor'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 51\u001b[0m\n\u001b[1;32m     41\u001b[0m tuner\u001b[38;5;241m.\u001b[39msearch(\n\u001b[1;32m     42\u001b[0m     x\u001b[38;5;241m=\u001b[39mtrain_data[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     43\u001b[0m     y\u001b[38;5;241m=\u001b[39mtrain_data[\u001b[38;5;241m1\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m\n\u001b[1;32m     47\u001b[0m )\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Retrieve the best model and hyperparameters\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_models\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     52\u001b[0m best_hyperparameters \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mget_best_hyperparameters(num_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Print the best hyperparameters\u001b[39;00m\n",
      "File \u001b[0;32m~/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/tuner.py:400\u001b[0m, in \u001b[0;36mTuner.get_best_models\u001b[0;34m(self, num_models)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the best model(s), as determined by the tuner's objective.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \n\u001b[1;32m    384\u001b[0m \u001b[38;5;124;03mThe models are loaded with the weights corresponding to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    List of trained model instances sorted from the best to the worst.\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# Method only exists in this class for the docstring override.\u001b[39;00m\n\u001b[0;32m--> 400\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_models\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/base_tuner.py:366\u001b[0m, in \u001b[0;36mBaseTuner.get_best_models\u001b[0;34m(self, num_models)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the best model(s), as determined by the objective.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03mThis method is for querying the models trained during the search.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    List of trained models sorted from the best to the worst.\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    365\u001b[0m best_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_best_trials(num_models)\n\u001b[0;32m--> 366\u001b[0m models \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_model(trial) \u001b[38;5;28;01mfor\u001b[39;00m trial \u001b[38;5;129;01min\u001b[39;00m best_trials]\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m models\n",
      "File \u001b[0;32m~/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/base_tuner.py:366\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the best model(s), as determined by the objective.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03mThis method is for querying the models trained during the search.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    List of trained models sorted from the best to the worst.\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    365\u001b[0m best_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_best_trials(num_models)\n\u001b[0;32m--> 366\u001b[0m models \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m trial \u001b[38;5;129;01min\u001b[39;00m best_trials]\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m models\n",
      "File \u001b[0;32m~/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/tuner.py:320\u001b[0m, in \u001b[0;36mTuner.load_model\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial):\n\u001b[0;32m--> 320\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;66;03m# Build model to create the weights.\u001b[39;00m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmulti_backend() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model\u001b[38;5;241m.\u001b[39mbuilt:\n",
      "File \u001b[0;32m~/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/tuner.py:164\u001b[0m, in \u001b[0;36mTuner._try_build\u001b[0;34m(self, hp)\u001b[0m\n\u001b[1;32m    161\u001b[0m keras\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mclear_session()\n\u001b[1;32m    162\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m--> 164\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_hypermodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# Stop if `build()` does not return a valid model.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, keras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mModel):\n",
      "File \u001b[0;32m~/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/tuner.py:155\u001b[0m, in \u001b[0;36mTuner._build_hypermodel\u001b[0;34m(self, hp)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_hypermodel\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp):\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m maybe_distribute(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribution_strategy):\n\u001b[0;32m--> 155\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhypermodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_override_compile_args(model)\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m, in \u001b[0;36mbuild_bert_model\u001b[0;34m(hp)\u001b[0m\n\u001b[1;32m     11\u001b[0m dense \u001b[38;5;241m=\u001b[39m Dense(units\u001b[38;5;241m=\u001b[39mhp\u001b[38;5;241m.\u001b[39mInt(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munits\u001b[39m\u001b[38;5;124m\"\u001b[39m, min_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, max_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)(pooled_output)\n\u001b[1;32m     12\u001b[0m output \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m)(dense)\n\u001b[0;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m hp\u001b[38;5;241m.\u001b[39mChoice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m, values\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1e-5\u001b[39m, \u001b[38;5;241m2e-5\u001b[39m, \u001b[38;5;241m3e-5\u001b[39m])\n\u001b[1;32m     17\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate)\n",
      "File \u001b[0;32m~/AI/Dissertation/.venv/lib/python3.9/site-packages/keras/src/utils/tracking.py:26\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m DotNotTrackScope():\n\u001b[0;32m---> 26\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI/Dissertation/.venv/lib/python3.9/site-packages/keras/src/models/functional.py:119\u001b[0m, in \u001b[0;36mFunctional.__init__\u001b[0;34m(self, inputs, outputs, name, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m flat_inputs:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, backend\u001b[38;5;241m.\u001b[39mKerasTensor):\n\u001b[0;32m--> 119\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    120\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll `inputs` values must be KerasTensors. Received: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    121\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m including invalid value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         )\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m flat_outputs:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, backend\u001b[38;5;241m.\u001b[39mKerasTensor):\n",
      "\u001b[0;31mValueError\u001b[0m: All `inputs` values must be KerasTensors. Received: inputs=[<tf.Tensor: shape=(1000, 50), dtype=int32, numpy=\narray([[  101, 23331,  2190, ...,     0,     0,     0],\n       [  101,  2028,  2190, ...,  1012,  2065,   102],\n       [  101, 15736,  2080, ...,  2909,  3726,   102],\n       ...,\n       [  101,  2625,  2093, ...,     0,     0,     0],\n       [  101,  4149,  2047, ...,     0,     0,     0],\n       [  101,  2299,  4276, ...,  4602,  4978,   102]], dtype=int32)>, <tf.Tensor: shape=(1000, 50), dtype=int32, numpy=\narray([[1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 1, 1, 1],\n       [1, 1, 1, ..., 1, 1, 1],\n       ...,\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 1, 1, 1]], dtype=int32)>] including invalid value [[  101 23331  2190 ...     0     0     0]\n [  101  2028  2190 ...  1012  2065   102]\n [  101 15736  2080 ...  2909  3726   102]\n ...\n [  101  2625  2093 ...     0     0     0]\n [  101  4149  2047 ...     0     0     0]\n [  101  2299  4276 ...  4602  4978   102]] of type <class 'tensorflow.python.framework.ops.EagerTensor'>"
     ]
    }
   ],
   "source": [
    "def build_bert_model(hp):\n",
    "    input_ids = X_train_tokens[\"input_ids\"]\n",
    "    attention_mask = X_train_tokens[\"attention_mask\"]\n",
    "    print('input_ids',type(input_ids))\n",
    "    print('attention_mask',type(attention_mask))\n",
    "    \n",
    "    bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    bert_output = bert_model(input_ids, attention_mask=attention_mask)[0] \n",
    "    pooled_output = GlobalAveragePooling1D()(bert_output)  \n",
    "    \n",
    "    dense = Dense(units=hp.Int(\"units\", min_value=32, max_value=128, step=16), activation='relu')(pooled_output)\n",
    "    output = Dense(1, activation='softmax')(dense)\n",
    "    \n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "    \n",
    "    learning_rate = hp.Choice(\"learning_rate\", values=[1e-5, 2e-5, 3e-5])\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.build()\n",
    "    model.compile(optimizer=optimizer, loss=SparseCategoricalCrossentropy(), metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_bert_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=2,\n",
    "    executions_per_trial=1,\n",
    "    directory=\"bert_tuning\",\n",
    "    project_name=\"bert_sentiment_analysis\"\n",
    ")\n",
    "\n",
    "train_data = (\n",
    "    {\"input_ids\": X_train_tokens[\"input_ids\"], \"attention_mask\": X_train_tokens[\"attention_mask\"]},\n",
    "    y_train\n",
    ")\n",
    "val_data = (\n",
    "    {\"input_ids\": X_test_tokens[\"input_ids\"], \"attention_mask\": X_test_tokens[\"attention_mask\"]},\n",
    "    y_test\n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    x=train_data[0],\n",
    "    y=train_data[1],\n",
    "    validation_data=val_data,\n",
    "    epochs=3,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "\n",
    "# Retrieve the best model and hyperparameters\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(f\"Learning Rate: {best_hyperparameters.get('learning_rate')}\")\n",
    "# print(f\"Weight Decay: {best_hyperparameters.get('weight_decay')}\")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model.evaluate(\n",
    "    {'input_ids': X_test_tokens['input_ids'], 'attention_mask': X_test_tokens['attention_mask']},\n",
    "    y_test\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 6s 85ms/step\n",
      "Model Accuracy: 86.95%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.83      0.85       893\n",
      "           1       0.87      0.90      0.88      1107\n",
      "\n",
      "    accuracy                           0.87      2000\n",
      "   macro avg       0.87      0.87      0.87      2000\n",
      "weighted avg       0.87      0.87      0.87      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict with the best model\n",
    "y_pred_logits = best_model.predict({'input_ids': X_test_tokens['input_ids'], 'attention_mask': X_test_tokens['attention_mask']})\n",
    "y_pred = np.argmax(y_pred_logits, axis=-1)\n",
    "# Model evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = os.path.join(INPUT_DIR, 'bert_best_model.h5')\n",
    "params_filename = os.path.join(INPUT_DIR,'bert_best_params.pkl')\n",
    "\n",
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "# Save the best hyperparameters and model after tuning\n",
    "def save_model_and_params(best_model, best_params, model_filename=model_filename, params_filename=params_filename):\n",
    "    # Save best model\n",
    "    best_model.save(model_filename)\n",
    "        \n",
    "    # Save best hyperparameters\n",
    "    with open(params_filename, 'wb') as params_file:\n",
    "        pickle.dump(best_params, params_file)\n",
    "\n",
    "# Example usage (after tuning)\n",
    "save_model_and_params(best_model, best_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = os.path.join(INPUT_DIR, 'bert_best_model.h5')\n",
    "params_filename = os.path.join(INPUT_DIR,'bert_best_params.pkl')\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the best hyperparameters and model from the saved files\n",
    "def load_model_and_params(model_filename=model_filename, params_filename=params_filename):\n",
    "    # Load best hyperparameters\n",
    "    with open(params_filename, 'rb') as params_file:\n",
    "        best_params = pickle.load(params_file)\n",
    "\n",
    "    # Load best model using custom object scope for TFBertModel\n",
    "    with custom_object_scope({'TFBertModel': TFBertModel}):\n",
    "        best_model = load_model(model_filename)\n",
    "            \n",
    "\n",
    "    return  best_model, best_params\n",
    "\n",
    "# Example usage\n",
    "best_model, best_params = load_model_and_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_2 (TFBertModel)  TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 100,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1 (Gl  (None, 768)         0           ['tf_bert_model_2[0][0]']        \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 68)           52292       ['global_average_pooling1d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 2)            138         ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,534,670\n",
      "Trainable params: 109,534,670\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build a new BERT model with the loaded hyperparameters\n",
    "def build_bert_model_with_params(best_params):\n",
    "\n",
    "    input_ids = Input(shape=(100,), dtype='int32', name=\"input_ids\")\n",
    "    attention_mask = Input(shape=(100,), dtype='int32', name=\"attention_mask\")\n",
    "    \n",
    "    bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    bert_output = bert_model(input_ids, attention_mask=attention_mask)[0] \n",
    "    pooled_output = GlobalAveragePooling1D()(bert_output)  \n",
    "    \n",
    "    dense = Dense(units=68, activation='relu')(pooled_output)\n",
    "    output = Dense(2, activation='softmax')(dense)\n",
    "    \n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "    # # Create a new BERT model with the best hyperparameters\n",
    "    # bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Compile the model using the best hyperparameters\n",
    "    learning_rate = best_params.get('learning_rate')  # Default value if not found\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Build a new model with the best hyperparameters\n",
    "best_model = build_bert_model_with_params(best_params)\n",
    "\n",
    "# Print the model summary\n",
    "best_model.summary()\n",
    "\n",
    "model_filename = os.path.join(INPUT_DIR, 'bert_best_model.h5')\n",
    "best_model.save(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "313/313 [==============================] - 90s 243ms/step - loss: 0.4042 - accuracy: 0.4703 - val_loss: 0.3152 - val_accuracy: 0.4715\n",
      "Epoch 2/2\n",
      "313/313 [==============================] - 75s 238ms/step - loss: 0.2347 - accuracy: 0.4707 - val_loss: 0.3232 - val_accuracy: 0.4710\n"
     ]
    }
   ],
   "source": [
    "# Train the loaded model with the same dataset\n",
    "history = best_model.fit(\n",
    "    {'input_ids': X_train_tokens['input_ids'], 'attention_mask': X_train_tokens['attention_mask']},\n",
    "    y_train,\n",
    "    validation_data=({'input_ids': X_test_tokens['input_ids'], 'attention_mask': X_test_tokens['attention_mask']}, y_test),\n",
    "    epochs=2,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 7s 85ms/step\n",
      "Model Accuracy: 86.05%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85       893\n",
      "           1       0.89      0.85      0.87      1107\n",
      "\n",
      "    accuracy                           0.86      2000\n",
      "   macro avg       0.86      0.86      0.86      2000\n",
      "weighted avg       0.86      0.86      0.86      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict with the best model\n",
    "y_pred_logits = best_model.predict({'input_ids': X_test_tokens['input_ids'], 'attention_mask': X_test_tokens['attention_mask']})\n",
    "y_pred = np.argmax(y_pred_logits, axis=-1)\n",
    "# Model evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a function to plot training history\n",
    "def plot_training_history(history, title=\"Model Training History\"):\n",
    "    # Extract values from history\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    \n",
    "    # Plot training and validation accuracy\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, 'b', label='Training Accuracy')\n",
    "    plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
    "    plt.title(f\"{title} - Accuracy\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, 'b', label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "    plt.title(f\"{title} - Loss\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for hist\n",
    "plot_training_history(hist, title=\"Initial Model Training History\")\n",
    "\n",
    "# Plot for history\n",
    "plot_training_history(history, title=\"Searched Model Training History\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
