{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Libs for Randomsearch]\n",
    "TensorFlow version: 2.11.0\n",
    "Transformers version: 4.17.0\n",
    "Keras version: 2.11.0\n",
    "[Libs for Bert]\n",
    "TensorFlow version: 2.17.0\n",
    "Transformers version: 4.45.2\n",
    "Keras version: 3.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.17.0\n",
      "Transformers version: 4.45.2\n",
      "Keras version: 3.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import transformers\n",
    "import keras\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification, AdamWeightDecay, TFBertModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling1D\n",
    "\n",
    "from tensorflow.keras.utils import custom_object_scope\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "# from keras_tuner import HyperParameters, RandomSearch\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "INPUT_DIR = \"../Output/proto_models_rev2_1000\"\n",
    "# Load data\n",
    "df_train = pd.read_csv(os.path.join(INPUT_DIR, 'train_cleaned.csv'))\n",
    "df_test = pd.read_csv(os.path.join(INPUT_DIR, 'test_cleaned.csv'))\n",
    "# df_test = processor.load_data()\n",
    "X_train = df_train['review']\n",
    "X_test = df_test['review']\n",
    "y_train = df_train['polarity']\n",
    "y_test = df_test['polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenizing the datasets\n",
    "X_train_tokens = tokenizer(\n",
    "    text=list(X_train),\n",
    "    add_special_tokens=True,\n",
    "    max_length=100,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True\n",
    ")\n",
    "\n",
    "X_test_tokens = tokenizer(\n",
    "    text=list(X_test),\n",
    "    add_special_tokens=True,\n",
    "    max_length=100,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0 (unused)\n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109483778 (417.65 MB)\n",
      "Trainable params: 109483778 (417.65 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "32/32 [==============================] - 17s 280ms/step - loss: 0.6099 - accuracy: 0.6640 - val_loss: 0.4947 - val_accuracy: 0.7800\n",
      "Epoch 2/3\n",
      "32/32 [==============================] - 8s 249ms/step - loss: 0.3987 - accuracy: 0.8300 - val_loss: 0.2895 - val_accuracy: 0.9000\n",
      "Epoch 3/3\n",
      "32/32 [==============================] - 8s 248ms/step - loss: 0.2480 - accuracy: 0.9140 - val_loss: 0.1620 - val_accuracy: 0.9550\n",
      "7/7 [==============================] - 2s 77ms/step\n"
     ]
    }
   ],
   "source": [
    "# Define BERT Model\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Use Hugging Face's AdamWeightDecay optimizer\n",
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n",
    "# Compile the model using a standard loss function\n",
    "loss_fn = SparseCategoricalCrossentropy(from_logits=True)\n",
    "bert_model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "# Model Summary\n",
    "bert_model.summary()\n",
    "\n",
    "# Train the model\n",
    "hist = bert_model.fit(\n",
    "    {'input_ids': X_train_tokens['input_ids'], 'attention_mask': X_train_tokens['attention_mask']},\n",
    "    y_train,\n",
    "    validation_data=({'input_ids': X_test_tokens['input_ids'], 'attention_mask': X_test_tokens['attention_mask']}, y_test),\n",
    "    epochs=3,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_logits = bert_model.predict({'input_ids': X_test_tokens['input_ids'], 'attention_mask': X_test_tokens['attention_mask']}).logits\n",
    "y_pred = np.argmax(y_pred_logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 95.50%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.96       102\n",
      "           1       0.97      0.94      0.95        98\n",
      "\n",
      "    accuracy                           0.95       200\n",
      "   macro avg       0.96      0.95      0.95       200\n",
      "weighted avg       0.96      0.95      0.95       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Tensorflow version 2.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from bert_tuning/bert_sentiment_analysis/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "def build_bert_model(hp):\n",
    "    input_ids = Input(shape=(100,), dtype='int32', name=\"input_ids\")\n",
    "    attention_mask = Input(shape=(100,), dtype='int32', name=\"attention_mask\")\n",
    "    \n",
    "    bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    bert_output = bert_model(input_ids, attention_mask=attention_mask)[0] \n",
    "    pooled_output = GlobalAveragePooling1D()(bert_output)  \n",
    "    \n",
    "    dense = Dense(units=hp.Int(\"units\", min_value=32, max_value=128, step=16), activation='relu')(pooled_output)\n",
    "    output = Dense(2, activation='softmax')(dense)\n",
    "    \n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "    \n",
    "    learning_rate = hp.Choice(\"learning_rate\", values=[1e-5, 2e-5, 3e-5])\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.build()\n",
    "    model.compile(optimizer=optimizer, loss=SparseCategoricalCrossentropy(), metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_bert_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=20,\n",
    "    executions_per_trial=1,\n",
    "    directory=\"bert_tuning\",\n",
    "    project_name=\"bert_sentiment_analysis\"\n",
    ")\n",
    "\n",
    "train_data = (\n",
    "    {\"input_ids\": X_train_tokens[\"input_ids\"], \"attention_mask\": X_train_tokens[\"attention_mask\"]},\n",
    "    y_train\n",
    ")\n",
    "val_data = (\n",
    "    {\"input_ids\": X_test_tokens[\"input_ids\"], \"attention_mask\": X_test_tokens[\"attention_mask\"]},\n",
    "    y_test\n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    x=train_data[0],\n",
    "    y=train_data[1],\n",
    "    validation_data=val_data,\n",
    "    epochs=3,\n",
    "    batch_size=32\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in bert_tuning/bert_sentiment_analysis\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 0 summary\n",
      "Hyperparameters:\n",
      "units: 64\n",
      "learning_rate: 2e-05\n",
      "Score: 0.8694999814033508\n",
      "\n",
      "Trial 4 summary\n",
      "Hyperparameters:\n",
      "units: 128\n",
      "learning_rate: 2e-05\n",
      "Score: 0.8684999942779541\n",
      "\n",
      "Trial 3 summary\n",
      "Hyperparameters:\n",
      "units: 96\n",
      "learning_rate: 2e-05\n",
      "Score: 0.8675000071525574\n",
      "\n",
      "Trial 05 summary\n",
      "Hyperparameters:\n",
      "units: 128\n",
      "learning_rate: 3e-05\n",
      "Score: 0.8675000071525574\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "units: 32\n",
      "learning_rate: 1e-05\n",
      "Score: 0.8615000247955322\n",
      "\n",
      "Trial 09 summary\n",
      "Hyperparameters:\n",
      "units: 128\n",
      "learning_rate: 1e-05\n",
      "Score: 0.8600000143051147\n",
      "\n",
      "Trial 06 summary\n",
      "Hyperparameters:\n",
      "units: 32\n",
      "learning_rate: 2e-05\n",
      "Score: 0.8585000038146973\n",
      "\n",
      "Trial 10 summary\n",
      "Hyperparameters:\n",
      "units: 96\n",
      "learning_rate: 1e-05\n",
      "Score: 0.8585000038146973\n",
      "\n",
      "Trial 08 summary\n",
      "Hyperparameters:\n",
      "units: 64\n",
      "learning_rate: 1e-05\n",
      "Score: 0.8550000190734863\n",
      "\n",
      "Trial 2 summary\n",
      "Hyperparameters:\n",
      "units: 32\n",
      "learning_rate: 3e-05\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/base_tuner.py\", line 274, in _try_run_and_update_trial\n",
      "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/base_tuner.py\", line 239, in _run_and_update_trial\n",
      "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n",
      "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
      "  File \"/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/tuner.py\", line 232, in _build_and_fit_model\n",
      "    model = self._try_build(hp)\n",
      "  File \"/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/tuner.py\", line 164, in _try_build\n",
      "    model = self._build_hypermodel(hp)\n",
      "  File \"/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/tuner.py\", line 155, in _build_hypermodel\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"/tmp/ipykernel_30378/1091352493.py\", line 6, in build_bert_model\n",
      "    bert_output = bert_model(input_ids, attention_mask=attention_mask)[0]  # 마지막 은닉층의 출력\n",
      "  File \"/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/tf_keras/src/utils/traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 436, in run_call_with_unpacked_inputs\n",
      "    unpacked_inputs = input_processing(func, config, **fn_args_and_kwargs)\n",
      "  File \"/home/woong/AI/Dissertation/.venv/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 513, in input_processing\n",
      "    raise ValueError(f\"Data of type {type(v)} is not allowed only {allowed_types} is accepted for {k}.\")\n",
      "ValueError: Exception encountered when calling layer 'tf_bert_model' (type TFBertModel).\n",
      "\n",
      "Data of type <class 'keras.src.backend.common.keras_tensor.KerasTensor'> is not allowed only (<class 'tensorflow.python.framework.tensor.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.utils.generic.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>, <class 'numpy.ndarray'>) is accepted for attention_mask.\n",
      "\n",
      "Call arguments received by layer 'tf_bert_model' (type TFBertModel):\n",
      "  • input_ids=<KerasTensor shape=(None, 100), dtype=int32, sparse=False, name=input_ids>\n",
      "  • attention_mask=<KerasTensor shape=(None, 100), dtype=int32, sparse=False, name=attention_mask>\n",
      "  • token_type_ids=None\n",
      "  • position_ids=None\n",
      "  • head_mask=None\n",
      "  • inputs_embeds=None\n",
      "  • encoder_hidden_states=None\n",
      "  • encoder_attention_mask=None\n",
      "  • past_key_values=None\n",
      "  • use_cache=None\n",
      "  • output_attentions=None\n",
      "  • output_hidden_states=None\n",
      "  • return_dict=None\n",
      "  • training=False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'tf_bert_model' (type TFBertModel).\n\nData of type <class 'keras.src.backend.common.keras_tensor.KerasTensor'> is not allowed only (<class 'tensorflow.python.framework.tensor.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.utils.generic.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>, <class 'numpy.ndarray'>) is accepted for attention_mask.\n\nCall arguments received by layer 'tf_bert_model' (type TFBertModel):\n  • input_ids=<KerasTensor shape=(None, 100), dtype=int32, sparse=False, name=input_ids>\n  • attention_mask=<KerasTensor shape=(None, 100), dtype=int32, sparse=False, name=attention_mask>\n  • token_type_ids=None\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • encoder_hidden_states=None\n  • encoder_attention_mask=None\n  • past_key_values=None\n  • use_cache=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Retrieve the best model and hyperparameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_models\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m best_hyperparameters \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mget_best_hyperparameters(num_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Print the best hyperparameters\u001b[39;00m\n",
      "File \u001b[0;32m~/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/tuner.py:400\u001b[0m, in \u001b[0;36mTuner.get_best_models\u001b[0;34m(self, num_models)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the best model(s), as determined by the tuner's objective.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \n\u001b[1;32m    384\u001b[0m \u001b[38;5;124;03mThe models are loaded with the weights corresponding to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m    List of trained model instances sorted from the best to the worst.\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# Method only exists in this class for the docstring override.\u001b[39;00m\n\u001b[0;32m--> 400\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_models\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/base_tuner.py:366\u001b[0m, in \u001b[0;36mBaseTuner.get_best_models\u001b[0;34m(self, num_models)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the best model(s), as determined by the objective.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03mThis method is for querying the models trained during the search.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    List of trained models sorted from the best to the worst.\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    365\u001b[0m best_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_best_trials(num_models)\n\u001b[0;32m--> 366\u001b[0m models \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_model(trial) \u001b[38;5;28;01mfor\u001b[39;00m trial \u001b[38;5;129;01min\u001b[39;00m best_trials]\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m models\n",
      "File \u001b[0;32m~/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/base_tuner.py:366\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the best model(s), as determined by the objective.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03mThis method is for querying the models trained during the search.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    List of trained models sorted from the best to the worst.\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    365\u001b[0m best_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_best_trials(num_models)\n\u001b[0;32m--> 366\u001b[0m models \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m trial \u001b[38;5;129;01min\u001b[39;00m best_trials]\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m models\n",
      "File \u001b[0;32m~/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/tuner.py:320\u001b[0m, in \u001b[0;36mTuner.load_model\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial):\n\u001b[0;32m--> 320\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyperparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;66;03m# Build model to create the weights.\u001b[39;00m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmulti_backend() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model\u001b[38;5;241m.\u001b[39mbuilt:\n",
      "File \u001b[0;32m~/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/tuner.py:164\u001b[0m, in \u001b[0;36mTuner._try_build\u001b[0;34m(self, hp)\u001b[0m\n\u001b[1;32m    161\u001b[0m keras\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mclear_session()\n\u001b[1;32m    162\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m--> 164\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_hypermodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# Stop if `build()` does not return a valid model.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, keras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mModel):\n",
      "File \u001b[0;32m~/AI/Dissertation/.venv/lib/python3.9/site-packages/keras_tuner/src/engine/tuner.py:155\u001b[0m, in \u001b[0;36mTuner._build_hypermodel\u001b[0;34m(self, hp)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_hypermodel\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp):\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m maybe_distribute(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribution_strategy):\n\u001b[0;32m--> 155\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhypermodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_override_compile_args(model)\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "Cell \u001b[0;32mIn[33], line 6\u001b[0m, in \u001b[0;36mbuild_bert_model\u001b[0;34m(hp)\u001b[0m\n\u001b[1;32m      3\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m100\u001b[39m,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m'\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m bert_model \u001b[38;5;241m=\u001b[39m TFBertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m bert_output \u001b[38;5;241m=\u001b[39m \u001b[43mbert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m] \n\u001b[1;32m      7\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m GlobalAveragePooling1D()(bert_output)  \n\u001b[1;32m      9\u001b[0m dense \u001b[38;5;241m=\u001b[39m Dense(units\u001b[38;5;241m=\u001b[39mhp\u001b[38;5;241m.\u001b[39mInt(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munits\u001b[39m\u001b[38;5;124m\"\u001b[39m, min_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, max_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)(pooled_output)\n",
      "File \u001b[0;32m~/AI/Dissertation/.venv/lib/python3.9/site-packages/tf_keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/AI/Dissertation/.venv/lib/python3.9/site-packages/transformers/modeling_tf_utils.py:436\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    434\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m--> 436\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m \u001b[43minput_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args_and_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munpacked_inputs)\n",
      "File \u001b[0;32m~/AI/Dissertation/.venv/lib/python3.9/site-packages/transformers/modeling_tf_utils.py:513\u001b[0m, in \u001b[0;36minput_processing\u001b[0;34m(func, config, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m         output[k] \u001b[38;5;241m=\u001b[39m v\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 513\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(v)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not allowed only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mallowed_types\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is accepted for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(main_input, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(main_input):\n\u001b[1;32m    517\u001b[0m         \u001b[38;5;66;03m# EagerTensors don't allow to use the .name property so we check for a real Tensor\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'tf_bert_model' (type TFBertModel).\n\nData of type <class 'keras.src.backend.common.keras_tensor.KerasTensor'> is not allowed only (<class 'tensorflow.python.framework.tensor.Tensor'>, <class 'bool'>, <class 'int'>, <class 'transformers.utils.generic.ModelOutput'>, <class 'tuple'>, <class 'list'>, <class 'dict'>, <class 'numpy.ndarray'>) is accepted for attention_mask.\n\nCall arguments received by layer 'tf_bert_model' (type TFBertModel):\n  • input_ids=<KerasTensor shape=(None, 100), dtype=int32, sparse=False, name=input_ids>\n  • attention_mask=<KerasTensor shape=(None, 100), dtype=int32, sparse=False, name=attention_mask>\n  • token_type_ids=None\n  • position_ids=None\n  • head_mask=None\n  • inputs_embeds=None\n  • encoder_hidden_states=None\n  • encoder_attention_mask=None\n  • past_key_values=None\n  • use_cache=None\n  • output_attentions=None\n  • output_hidden_states=None\n  • return_dict=None\n  • training=False"
     ]
    }
   ],
   "source": [
    "# Retrieve the best model and hyperparameters\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(f\"Learning Rate: {best_hyperparameters.get('learning_rate')}\")\n",
    "# print(f\"Weight Decay: {best_hyperparameters.get('weight_decay')}\")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model.evaluate(\n",
    "    {'input_ids': X_test_tokens['input_ids'], 'attention_mask': X_test_tokens['attention_mask']},\n",
    "    y_test\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 6s 85ms/step\n",
      "Model Accuracy: 86.95%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.83      0.85       893\n",
      "           1       0.87      0.90      0.88      1107\n",
      "\n",
      "    accuracy                           0.87      2000\n",
      "   macro avg       0.87      0.87      0.87      2000\n",
      "weighted avg       0.87      0.87      0.87      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict with the best model\n",
    "y_pred_logits = best_model.predict({'input_ids': X_test_tokens['input_ids'], 'attention_mask': X_test_tokens['attention_mask']})\n",
    "y_pred = np.argmax(y_pred_logits, axis=-1)\n",
    "# Model evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = os.path.join(INPUT_DIR, 'bert_best_model.h5')\n",
    "params_filename = os.path.join(INPUT_DIR,'bert_best_params.pkl')\n",
    "\n",
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "# Save the best hyperparameters and model after tuning\n",
    "def save_model_and_params(best_model, best_params, model_filename=model_filename, params_filename=params_filename):\n",
    "    # Save best model\n",
    "    best_model.save(model_filename)\n",
    "        \n",
    "    # Save best hyperparameters\n",
    "    with open(params_filename, 'wb') as params_file:\n",
    "        pickle.dump(best_params, params_file)\n",
    "\n",
    "# Example usage (after tuning)\n",
    "save_model_and_params(best_model, best_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = os.path.join(INPUT_DIR, 'bert_best_model.h5')\n",
    "params_filename = os.path.join(INPUT_DIR,'bert_best_params.pkl')\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the best hyperparameters and model from the saved files\n",
    "def load_model_and_params(model_filename=model_filename, params_filename=params_filename):\n",
    "    # Load best hyperparameters\n",
    "    with open(params_filename, 'rb') as params_file:\n",
    "        best_params = pickle.load(params_file)\n",
    "\n",
    "    # Load best model using custom object scope for TFBertModel\n",
    "    with custom_object_scope({'TFBertModel': TFBertModel}):\n",
    "        best_model = load_model(model_filename)\n",
    "            \n",
    "\n",
    "    return  best_model, best_params\n",
    "\n",
    "# Example usage\n",
    "best_model, best_params = load_model_and_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_2 (TFBertModel)  TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 100,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1 (Gl  (None, 768)         0           ['tf_bert_model_2[0][0]']        \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 68)           52292       ['global_average_pooling1d_1[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 2)            138         ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,534,670\n",
      "Trainable params: 109,534,670\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build a new BERT model with the loaded hyperparameters\n",
    "def build_bert_model_with_params(best_params):\n",
    "\n",
    "    input_ids = Input(shape=(100,), dtype='int32', name=\"input_ids\")\n",
    "    attention_mask = Input(shape=(100,), dtype='int32', name=\"attention_mask\")\n",
    "    \n",
    "    bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    bert_output = bert_model(input_ids, attention_mask=attention_mask)[0] \n",
    "    pooled_output = GlobalAveragePooling1D()(bert_output)  \n",
    "    \n",
    "    dense = Dense(units=68, activation='relu')(pooled_output)\n",
    "    output = Dense(2, activation='softmax')(dense)\n",
    "    \n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "    # # Create a new BERT model with the best hyperparameters\n",
    "    # bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Compile the model using the best hyperparameters\n",
    "    learning_rate = best_params.get('learning_rate')  # Default value if not found\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Build a new model with the best hyperparameters\n",
    "best_model = build_bert_model_with_params(best_params)\n",
    "\n",
    "# Print the model summary\n",
    "best_model.summary()\n",
    "\n",
    "model_filename = os.path.join(INPUT_DIR, 'bert_best_model.h5')\n",
    "best_model.save(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "313/313 [==============================] - 90s 243ms/step - loss: 0.4042 - accuracy: 0.4703 - val_loss: 0.3152 - val_accuracy: 0.4715\n",
      "Epoch 2/2\n",
      "313/313 [==============================] - 75s 238ms/step - loss: 0.2347 - accuracy: 0.4707 - val_loss: 0.3232 - val_accuracy: 0.4710\n"
     ]
    }
   ],
   "source": [
    "# Train the loaded model with the same dataset\n",
    "history = best_model.fit(\n",
    "    {'input_ids': X_train_tokens['input_ids'], 'attention_mask': X_train_tokens['attention_mask']},\n",
    "    y_train,\n",
    "    validation_data=({'input_ids': X_test_tokens['input_ids'], 'attention_mask': X_test_tokens['attention_mask']}, y_test),\n",
    "    epochs=2,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 7s 85ms/step\n",
      "Model Accuracy: 86.05%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85       893\n",
      "           1       0.89      0.85      0.87      1107\n",
      "\n",
      "    accuracy                           0.86      2000\n",
      "   macro avg       0.86      0.86      0.86      2000\n",
      "weighted avg       0.86      0.86      0.86      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predict with the best model\n",
    "y_pred_logits = best_model.predict({'input_ids': X_test_tokens['input_ids'], 'attention_mask': X_test_tokens['attention_mask']})\n",
    "y_pred = np.argmax(y_pred_logits, axis=-1)\n",
    "# Model evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a function to plot training history\n",
    "def plot_training_history(history, title=\"Model Training History\"):\n",
    "    # Extract values from history\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    epochs = range(1, len(acc) + 1)\n",
    "    \n",
    "    # Plot training and validation accuracy\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, 'b', label='Training Accuracy')\n",
    "    plt.plot(epochs, val_acc, 'r', label='Validation Accuracy')\n",
    "    plt.title(f\"{title} - Accuracy\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, 'b', label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "    plt.title(f\"{title} - Loss\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for hist\n",
    "plot_training_history(hist, title=\"Initial Model Training History\")\n",
    "\n",
    "# Plot for history\n",
    "plot_training_history(history, title=\"Searched Model Training History\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
